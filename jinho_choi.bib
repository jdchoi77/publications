%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Jinho Choi at 2019-10-09 00:05:46 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@techreport{he:19a,
	Abstract = {This paper presents new state-of-the-art models for three tasks, part-of-speech tagging, syntactic parsing, and semantic parsing, using the cutting-edge contextualized embedding framework known as BERT. For each task, we first replicate and simplify the current state-of-the-art approach to enhance its model efficiency. We then evaluate our simplified approaches on those three tasks using token embeddings generated by BERT. 12 datasets in both English and Chinese are used for our experiments. The BERT models outperform the previously best-performing models by 2.5% on average (7.5% for the most significant case). Moreover, an in-depth analysis on the impact of BERT embeddings is provided using self-attention, which helps understanding in this rich yet representation. All models and source codes are available in public so that researchers can improve upon and utilize them to establish strong baselines for the next decade.},
	Author = {He, Han and Choi, Jinho D.},
	Date-Added = {2019-08-19 15:41:16 -0400},
	Date-Modified = {2019-09-25 22:13:22 -0400},
	Institution = {arXiv},
	Keywords = {selected,emorynlp},
	Number = {1908.04943},
	Title = {{Establishing Strong Baselines for the New Decade: Sequence Tagging, Syntactic and Semantic Parsing with BERT}},
	Url = {https://arxiv.org/abs/1908.04943},
	Url_Paper = {https://arxiv.org/pdf/1908.04943.pdf},
	Year = {2019},
	Bdsk-Url-1 = {https://arxiv.org/abs/1908.04943}}

@inproceedings{yang:19b,
	Abstract = {This paper presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations. Each dialogue, involving multiple speakers, is annotated with several types of questions regarding the dialogue contexts, and the answers are annotated with certain spans in the dialogue. A series of crowdsourcing tasks are conducted to ensure good annotation quality, resulting a high inter-annotator agreement of 81.82%. A comprehensive annotation analytics is provided for a deeper understanding in this dataset. Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset. BERT in particular depicts promising results, an accuracy of 74.2% for answer utterance selection and an F1-score of 64.2% for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.},
	Author = {Yang, Zhengzhe and Choi, Jinho D.},
	Booktitle = {Proceedings of the Annual Conference of the ACL Special Interest Group on Discourse and Dialogue},
	Date-Added = {2019-07-05 14:43:48 -0400},
	Date-Modified = {2019-10-09 00:05:36 -0400},
	Keywords = {selected,emorynlp,character-mining},
	Series = {SIGDIAL'19},
	Title = {{FriendsQA: Open-Domain Question Answering on TV Show Transcripts}},
	Url = {https://www.sigdial.org/files/workshops/conference20/},
	Url_Paper = {https://www.aclweb.org/anthology/W19-5923.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/friendsqa-opendomain-question-answering-on-tv-show-transcripts-154329602},
	Year = {2019},
	Bdsk-Url-1 = {https://www.sigdial.org/files/workshops/conference20/index.html}}

@jurthesis{li:19a,
	Abstract = {Nowadays, manual diagnosis of early stages of neurodegenerative disorders such as Alzheimer's disease (AD) has been a challenge. While current neuropsychological examinations often fail to provide satisfactory result in detecting Mild Cognitive Impairment (MC) and linguistic ability has shown to be a good indication of symptoms of AD, in this thesis I examine the semantic linguistic features resulting from verbal utterances of potential patients to distinguish healthy people and people with the disease. For this purpose, I perform statistical and machine learning analysis on a specific language transcript dataset, consisting of 50 healthy people and 50 probable MCIs. Experimental and statistical evaluations suggest that certain patterns and semantic features are effective in helping the clinical diagnosis of MCI.},
	Address = {Atlanta, GA},
	Author = {Li, Mengmei},
	Date-Added = {2019-05-28 14:10:20 -0400},
	Date-Modified = {2019-07-05 14:59:00 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2019.},
	School = {Emory University},
	Title = {{Design of Meta-semantic Analysis for Automatic Detection of Alzheimer's Disease}},
	Year = {2019},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{yang:19a,
	Abstract = {This thesis presents FriendsQA, a challenging question answering dataset that contains 1,222 dialogues and 10,610 open-domain questions, to tackle machine comprehension on everyday conversations. Each dialogue, involving multiple speakers, is annotated with six types of questions {what, when, why, where, who, how} regarding the dialogue contexts, and the answers are annotated with contiguous spans in the dialogue. A series of crowdsourcing tasks are conducted to ensure good annotation quality, resulting a high inter-annotator agreement of 81.82%. A comprehensive annotation analytics is provided for a deeper understanding in this dataset. Three state-of-the-art QA systems are experimented, R-Net, QANet, and BERT, and evaluated on this dataset. BERT in particular depicts promising results, an accuracy of 74.2% for answer utterance selection and an F1-score of 64.2% for answer span selection, suggesting that the FriendsQA task is hard yet has a great potential of elevating QA research on multiparty dialogue to another level.},
	Address = {Atlanta, GA},
	Author = {Yang, Zhengzhe},
	Date-Added = {2019-05-28 14:09:35 -0400},
	Date-Modified = {2019-09-26 15:17:13 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2019.},
	School = {Emory University},
	Title = {{FriendsQA: Open-domain Question Answering on TV Show Transcripts}},
	Url = {https://etd.library.emory.edu/concern/etds/4q77fs51r},
	Url_Slides = {https://www.slideshare.net/jchoi7s/friendsqa-opendomain-question-answering-on-tv-show-transcripts-154329602},
	Year = {2019},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{jiang:19a,
	Abstract = {In this work, we propose a new training procedure for learning the importance of dimensions of word embeddings in representing word meanings. Our algorithm advanced in the interpretation filed of word embeddings, which are extremely critical in the NLP filed due to the lack of understanding of word embeddings despite their superior ability in progressing NLP tasks. Although previous work has investigated in the interpretability of word embeddings through imparting interpretability to the embedding training models or through post- processing procedures of pre-trained embeddings, our algorithm proposes a new perspective to word embedding dimension interpretation where each dimension gets evaluated and can be visualized. Also, our algorithm adheres to a novel assumption that not all dimensions are necessary for representing a word sense (word meaning) and dimensions that are negligible get discarded, which have not been attempted in previous studies.},
	Address = {Atlanta, GA},
	Author = {Jiang, Xinyi},
	Date-Added = {2019-05-28 14:09:03 -0400},
	Date-Modified = {2019-07-05 15:06:58 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2019.},
	School = {Emory University},
	Title = {{Incremental Sense Weight Training for Contextualized Word Embedding Interpretation}},
	Year = {2019},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{gao:19a,
	Abstract = {This thesis presents the design and architecture of an Active Learning system for Question Answering on Multiparty Dialogue. The goal of this system is to collect a robust Question Answering dataset and to improve the performance of the system on Question Answering challenges on Multiparty Dialogue. The system has an interactive web-based user interface which allows users to challenge the system with their own questions regarding a short passage of dialogues between multiple characters in a TV series. This system makes use of a state-of-art Machine Learning model to predict the answers to users' questions. In the same time, the system learns from users' responses and performs online update on the model. The system uses probability functions to guide user towards contributing data needed most for model improvement. The system is designed to handle heavy internet traffic by efficiently storing data and by carefully synchronizing the shared resources in the web system. The system has shown promising results in guiding users to contribute high quality data useful for model training.},
	Address = {Atlanta, GA},
	Author = {Gao, Shen},
	Date-Added = {2019-05-28 14:08:34 -0400},
	Date-Modified = {2019-09-26 15:20:50 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2019.},
	School = {Emory University},
	Title = {{Cloud-based Active Learning System for Question Answering on Multiparty Dialogue}},
	Url_Slides = {https://www.slideshare.net/jchoi7s/active-learning-on-question-answering-with-dialogues},
	Year = {2019},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{coves:19a,
	Abstract = {This paper introduces the first plural end-to-end coreference resolution model. This coreference system generates spans embeddings, which are optimized to predict the mentions and the coreferent antecedents. This model handles plural mentions and plural speakers. Our approach builds on the higher-order coreference resolution with coarse-to-fine inference by adapting it to the Friends corpus, which has plural speakers as a feature and also has singletons. Additionally, the model predicts plural antecedents as done in previous plural coreference works. These, in combination with the singular antecedents, are used to construct the final clusters, which have a one-to-one correspondence to the entities.},
	Address = {Atlanta, GA},
	Author = {Coves, Jose},
	Date-Added = {2019-05-28 14:05:06 -0400},
	Date-Modified = {2019-07-08 11:24:30 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2019.},
	School = {Emory University},
	Title = {{End-to-End Plural Coreference Resolution on TV Show Transcripts}},
	Url_Slides = {https://www.slideshare.net/jchoi7s/endtoend-plural-coreference-resolution-on-tv-show-transcripts},
	Year = {2019},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@inproceedings{choi:19a,
	Abstract = {This paper presents a new task-oriented meaning representation called meta-semantics, that is designed to detect patients with early symptoms of Alzheimer's disease by analyzing their language beyond a syntactic or semantic level. Meta-semantic representation consists of three parts, entities, predicate argument structures, and discourse attributes, that derive rich knowl- edge graphs. For this study, 50 controls and 50 patients with mild cognitive impairment (MCI) are selected, and meta-semantic representation is annotated on their speeches transcribed in text. Inter-annotator agreement scores of 88%, 82%, and 89% are achieved for the three types of annotation, respectively. Five analyses are made using this annotation, depicting clear dis- tinctions between the control and MCI groups. Finally, a neural model is trained on features extracted from those analyses to classify MCI patients from normal controls, showing a high accuracy of 82% that is very promising.},
	Author = {Choi, Jinho D. and Li, Mengmei and Goldstein, Felicia and Hajjar, Ihab},
	Booktitle = {Proceedings of the ACL Workshop on Designing Meaning Representations},
	Date-Added = {2019-05-28 11:11:08 -0400},
	Date-Modified = {2019-09-26 15:01:55 -0400},
	Keywords = {emorynlp,selected,medical},
	Pages = {82---91},
	Series = {DMR'19},
	Title = {{Meta-Semantic Representation for Early Detection of Alzheimer's Disease}},
	Url = {https://www.cs.brandeis.edu/~clp/dmr/},
	Url_Paper = {https://www.aclweb.org/anthology/W19-3309.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/metasemantic-representation-for-early-detection-of-alzheimers-disease},
	Year = {2019},
	Bdsk-Url-1 = {https://www.cs.brandeis.edu/~clp/dmr/}}

@inproceedings{shin:19b,
	Abstract = {Recent advances in deep learning have facilitated the demand of neural models for real applications. In practice, these applications often need to be deployed with limited resources while keeping high accuracy. This paper touches the core of neural models in NLP, word embeddings, and presents a new embedding distillation framework that remarkably reduces the dimension of word embeddings without compromising accuracy. A novel distillation ensemble approach is also proposed that trains a high-efficient student model using multiple teacher models. In our approach, the teacher models play roles only during training such that the student model operates on its own without getting supports from the teacher models during decoding, which makes it eighty times faster and lighter than other typical ensemble methods. All models are evaluated on seven document classification datasets and show significant advantage over the teacher models for most cases. Our analysis depicts insightful transformation of word embeddings from distillation and suggests a future direction to ensemble approaches using neural models.},
	Author = {Shin, Bonggun and Yang, Hao and Choi, Jinho D.},
	Booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
	Date-Added = {2019-05-10 13:20:57 -0400},
	Date-Modified = {2019-09-26 15:03:38 -0400},
	Keywords = {selected,emorynlp},
	Pages = {3439--3445},
	Series = {IJCAI'19},
	Title = {{The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning}},
	Url = {https://www.ijcai19.org/},
	Url_Paper = {https://www.ijcai.org/proceedings/2019/0477.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/the-pupil-has-become-the-master-teacherstudent-modelbased-word-embedding-distillation-with-ensemble-learning},
	Year = {2019},
	Bdsk-Url-1 = {https://www.ijcai19.org/}}

@phdthesis{jurczyk:17b,
	Abstract = {Question answering (QA) has lately gained lots of interest from both academic and industrial research. No matter the question, search engine users expect the machines to provide answers instantaneously, even without searching through relevant websites. While a significant portion of these questions ask for concise and well known facts, more complex questions do exist and they often require dedicated approaches to provide robust and accurate systems.

This thesis explores linguistically-oriented approaches for both factoid and non-factoid question answering and cross-genre text applications. The contributions include new annotation schemes for question answering oriented corpora, extracting linguistic structures and performing matching, and early exploration of conversation dialog text applications.

For sentence-based factoid question answering, a multi-stage crowdsourcing annotation scheme is presented. Next, a subtree matching algorithm for two sentences that aims to extract semantic similarity in open-domain texts is introduced and combined with a neural network architecture. Then, various factoid question answering corpora are thoroughly analyzed and cross-tested to improve the performance of QA systems. This thesis explores two complex scenarios of non-factoid question answering. In the first, a semantics-graph knowledge graph that is build on the top of linguistic structures is presented and applied on arithmetic questions using verb polarity classification. In the second, a system that combines lexical, syntactic and semantic text representations with statistical learning is presented and evaluated on event-based question answering. The last part of this thesis is focused on the cross-genre aspect of text in which the misalignment between the dialog and formal writings is the main challenge. First, an approach that combines semantic structure extraction with statistical learning is presented and used to improve the performance in the document retrieval task. Next, an exploration for the passage completion task is presented. A crowdsourcing annotation scheme is executed and a new corpus is created. A multi-gram convolutional neural network with the attention is compared to several state-of-the-art approaches for reading comprehension applications.},
	Author = {Jurczyk, Tomasz},
	Date-Added = {2019-03-13 23:04:33 -0400},
	Date-Modified = {2019-03-13 23:05:35 -0400},
	Keywords = {emorynlp},
	School = {Emory University},
	Title = {{Improving Question Answering by Bridging Linguistic Structures and Statistical Learning }},
	Url = {https://etd.library.emory.edu/concern/etds/xg94hp534},
	Year = {2017},
	Bdsk-Url-1 = {https://etd.library.emory.edu/concern/etds/xg94hp534}}

@inproceedings{shin:19a,
	Abstract = {Electronic Health Records (EHRs) have been heavily used to predict various downstream clinical tasks such as readmission or mortality. One of the modalities in EHRs, clinical notes, has not been fully explored for these tasks due to its unstructured and inexplicable nature. Although recent advances in deep learning (DL) enables models to extract interpretable features from unstructured data, they often require a large amount of training data. However, many tasks in medical domains inherently consist of small sample data with lengthy documents; for a kidney transplant as an example, data from only a few thousand of patients are available and each patient's document consists of a couple of millions of words in major hospitals. Thus, complex DL methods cannot be applied to these kind of domains. In this paper, we present a comprehensive ensemble model using vector space modeling and topic modeling. Our proposed model is evaluated on the readmission task of kidney transplant patients, and improves 0.0211 in terms of c-statistics from the previous state- of-the-art approach using structured data, while typical DL methods fails to beat this approach. The proposed architecture provides the interpretable score for each feature from both modalities, structured and unstructured data, which is shown to be meaningful through a physician's evaluation.},
	Author = {Shin, Bonggun and Hogan, Julien and Adams, Andrew B. and Lynch, Raymond J. and Patzer, Rachel E. and Choi, Jinho D.},
	Booktitle = {Proceedings of the IEEE-EMBS International Conference on Biomedical and Health Informatics},
	Date-Added = {2019-03-07 12:52:47 -0500},
	Date-Modified = {2019-09-26 15:04:24 -0400},
	Keywords = {emorynlp,selected,medical},
	Series = {BHI'19},
	Title = {{Multimodal Ensemble Approach to Incorporate Various Types of Clinical Notes for Predicting Readmission}},
	Url = {https://www.bhi-bsn-2019.org/bhi/},
	Url_Paper = {https://arxiv.org/pdf/1906.01498.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/multimodal-ensemble-approach-to-incorporate-various-types-of-clinical-notes-for-predicting-readmission},
	Year = {2019},
	Bdsk-Url-1 = {https://www.bhi-bsn-2019.org/}}

@phdthesis{jurczyk:18b,
	Abstract = {This thesis focuses on the optimization of NLP components for robustness and scalability. Three kinds of nlp components are used for our experiments, a part-of-speech tagger, a dependency parser, and a semantic role labeler. For part-of-speech tagging, dynamic model selection is introduced. Our dynamic model selection approach builds two models, domain-specific and generalized models, and selects one of them during decoding by comparing similarities between lexical items used for building these models and input sentences. As a result, it gives robust tagging accuracy across corpora and shows fast tagging speed. For dependency parsing, a new transition-based parsing algorithm and a bootstrapping technique are introduced. Our parsing algorithm learns both projective and non-projective transitions so it can generate both projective and non-projective dependency trees yet shows linear time parsing speed on average. Our bootstrapping technique bootstraps parse information used as features for transition-based parsing, and shows significant improvement for parsing accuracy. For semantic role labeling, a conditional higher-order argument pruning algorithm is introduced. A higher-order pruning algorithm improves the coverage of argument candidates and shows improvement on the overall F1-score. The conditional higher-order pruning algorithm also noticeably reduces average labeling complexity with minimal reduction in F1-score.

For all experiments, two sets of training data are used; one is from the Wall Street Journal corpus, and the other is from the OntoNotes corpora. All components are evaluated on 9 different genres, which are grouped separately for in-genre and out-of-genre experiments. Our experiments show that our approach gives higher accuracies compared to other state-of-the-art nlp components, and runs fast, taking about 3-4 milliseconds per sentence for processing all three components. All components are publicly available as an open source project, called ClearNLP. We believe that this project is beneficial for many nlp tasks that need to process large-scale heterogeneous data.},
	Author = {Jurczyk, Tomasz},
	Date-Added = {2018-08-21 21:48:13 +0000},
	Date-Modified = {2018-08-21 21:53:22 +0000},
	Keywords = {emorynlp},
	Organization = {Department of Mathematics and Computer Science},
	School = {Emory University},
	Title = {{Improving Question Answering by Bridging Linguistic Structures with Statistical Learning}},
	Url_Slides = {https://www.slideshare.net/jchoi7s/improving-question-answering-by-bridging-linguistic-structures-with-statistical-learning},
	Year = {2018},
	Bdsk-Url-1 = {http://www.mathcs.emory.edu/~choi/doc/thesis-2012.pdf}}

@jurthesis{zhou:18b,
	Address = {Atlanta, GA},
	Annote = {This paper analyzes arguably the most challenging yet under-explored aspect of resolution tasks such as coreference resolution and entity linking, that is the resolution of plural mentions. Unlike singular mentions each of which represents one entity, plural mentions stand for multiple entities. To tackle this aspect, we take the character identification corpus from the SemEval 2018 shared task that consists of entity annotation for singular mentions, and expand it by adding annotation for plural mentions. We then introduce a novel coreference resolution algorithm that selectively creates clusters to handle both singular and plural mentions, and also a deep learning-based entity linking model that jointly handles both types of mentions through multi-task learning. Adjusted evaluation metrics are proposed for these tasks as well to handle the uniqueness of plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks.},
	Author = {Zhou, Ethan},
	Date-Added = {2018-08-21 21:43:38 +0000},
	Date-Modified = {2019-05-29 20:48:58 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2018.},
	School = {Emory University},
	Title = {{A Thesis on Character Identification}},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/np1939186},
	Url_Slides = {https://www.slideshare.net/jchoi7s/a-thesis-on-character-identification},
	Year = {2018},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{ma:18b,
	Abstract = {This paper presents a new corpus and a robust deep learning architecture for a task in reading comprehension, passage completion, on multiparty dialog. Given a dialog in text and a passage containing factual descriptions about the dialog where mentions of the characters are replaced by blanks, the task is to fill the blanks with the most appropriate character names that reflect the contexts in the dialog. Since there is no dataset that challenges the task of passage completion in this genre, we create a corpus by selecting transcripts from a TV show that comprise 1,682 dialogs, generating passages for each dialog through crowdsourcing, and annotating mentions of characters in both the dialog and the passages. Given this dataset, we build a deep neural model that integrates rich feature extraction from convolutional neural networks into sequence modeling in recurrent neural networks, optimized by utterance and dialog level attentions. Our model outperforms the previ- ous state-of-the-art model on this task in a dif- ferent genre using bidirectional LSTM, showing a 13.0+% improvement for longer dialogs. Our analysis shows the effectiveness of the attention mechanisms and suggests a direction to machine comprehension on multiparty dialog.},
	Address = {Atlanta, GA},
	Author = {Ma, Kaixin},
	Date-Added = {2018-08-21 19:05:20 +0000},
	Date-Modified = {2018-08-21 19:07:28 +0000},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2018.},
	School = {Emory University},
	Title = {{Reading Comprehension on Multi-party Dialog}},
	Url_Slides = {https://www.slideshare.net/jchoi7s/reading-comprehension-on-multiparty-dialog},
	Year = {2018},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{chun:18b,
	Abstract = {This thesis gives two contributions in the form of lexical resourcesto (1) dependency parsing in Korean and (2) semantic parsing in English. First,we describe our methodology for building three dependency treebanks in Korean derived from existing treebanks and pseudo-annotated according to the latest guidelines from the Universal Dependencies (UD). The original Google Korean UD Treebank is re-tokenized to ensure morpheme-level annotation consistency with other corpora while maintaining linguistic validity of the revised tokens. Phrase structure trees in the Penn Korean Treebank and the Kaist Treebank are automatically converted into UD dependency trees by applying head-percolation rules and linguistically motivated heuristics. A total of 38K+ dependency trees are generated.To the best of our knowledge, this is the first time that the three Korean treebanks are converted into UD dependency treebanks following the latest annotation guidelines. Second, we introduce an on-going project for augmenting the OntoNotes phrase structure treebank with semantic features found in the Abstract Meaning Representation (AMR), as part of an effort to build an accurate AMR parser. We propose a novel technique for AMR parsing that first trains a dependency parser on the OntoNotes corpus augmented with numbered arguments in the Proposition Bank (PropBank), and then does a transfer learning of the trained dependency parser for the AMR parsing task. A preliminary step is to prepare dependency data by performing an automatic replacement of dependencies that define predicate argument structure with their corresponding PropBank argument labels during constituent-to-dependency conversion. To the best of our knowledge, this is the first time that the PropBank labels are directly inserted into dependency structure, producing a new dependency corpus with rich syntactic information as well as semantic role information provided by PropBank that fully describes the predicate-argument structure, making it an ideal resource for AMR parsing and, broadly, semantic parsing.},
	Address = {Atlanta, GA},
	Author = {Chun, Jayeol},
	Date-Added = {2018-08-21 18:54:53 +0000},
	Date-Modified = {2019-05-28 14:06:20 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2018.},
	School = {Emory University},
	Title = {{Dependency Analysis of Abstract Universal Structures in Korean and English}},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/sj1391960},
	Url_Slides = {https://www.slideshare.net/jchoi7s/monkeying-around-automatically-analyzing-malaria-infections-in-rhesus-macaques},
	Year = {2018},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@inproceedings{kanayama:18a,
	Abstract = {This paper discusses the representation of coordinate structures in the Universal Dependencies framework for two head-final languages, Japanese and Korean. UD applies a strict principle that makes the head of coordination the left-most conjunct. However, the guideline may produce syntactic trees which are difficult to accept in head-final languages. This paper describes the status in the current corpora and proposes alternative designs suitable for these languages.},
	Author = {Kanayama, Hiroshi and Han, Na-Rae and Asahara, Masayuki and Hwang, Jena D. and Miyao, Yusuke and Choi, Jinho D. and Matsumoto, Yuji},
	Booktitle = {Proceedings of the EMNLP Workshop on Universal Dependencies},
	Date-Added = {2018-08-20 21:52:25 +0000},
	Date-Modified = {2019-05-29 20:47:24 -0400},
	Keywords = {selected,emorynlp},
	Pages = {75--84},
	Series = {UDW'18},
	Title = {{Coordinate Structures in Universal Dependencies for Head-final Languages}},
	Url = {http://universaldependencies.org/udw18/},
	Url_Paper = {https://www.aclweb.org/anthology/W18-6009.pdf},
	Year = {2018},
	Bdsk-Url-1 = {http://universaldependencies.org/udw18/}}

@jurthesis{jiang:18a,
	Abstract = {Distributional Semantic word representation allows Natural Language Processing systems to extract and model an immense amount of information about a language. This technique maps words into a high dimensional continuous space through the use of a single-layer neural network. This process has allowed for advances in many Natural Language Processing research areas and tasks. These representation models are evaluated with the use of analogy tests, questions of the form ``If a is to a' then b is to what?'' are answered by composing multiple word vectors and searching the vector space.

During the neural network training process, each word is examined as a member of its context. Generally, a word's context is considered to be the elements adjacent to it within a sentence. While some work has been conducted examining the effect of expanding this definition, very little exploration has been done in this area. Further, no inquiry has been conducted as to the specific linguistic competencies of these models or whether modifying their contexts impacts the information they extract.

In this paper we propose a thorough analysis of the various lexical and grammatical competencies of distributional semantic models. We aim to leverage analogy tests to evaluate the most advanced distributional model across 14 different types of linguistic relationships. With this information we will then be able to investigate as to whether modifying the training context renders any differences in quality across any of these categories. Ideally we will be able to identify approaches to training that increase precision in some specific linguistic categories, which will allow us to investigate whether these improvements can be combined by joining the information used in different training approaches to build a single, improved, model.},
	Address = {Atlanta, GA},
	Author = {Jiang, Hang},
	Date-Added = {2018-07-10 17:06:27 +0000},
	Date-Modified = {2019-05-28 14:06:46 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2018.},
	School = {Emory University},
	Title = {{Automatic Personality Prediction with Attention-based Neural Networks}},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/rv042t11v},
	Url_Slides = {https://www.slideshare.net/jchoi7s/automatic-personality-prediction-with-attentionbased-neural-networks},
	Year = {2018},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{hexter:18a,
	Abstract = {In today's age of big data, automatic processing techniques are becoming more important than ever, especially in the field of biology and medicine. Many studies focus on genomic data, following the rise of high throughput sequencing; this project instead analyzes certain blood data parameters taken from rhesus macaques housed in Yerkes Primate Research Center at Emory University. The initial impetus for this study was the Joyner et al. 2016 paper, ``Plasmodium cynomolgi infections in rhesus macaques display clinical and parasitological features pertinent to modelling vivax malaria pathology and relapse infections" (Joyner et al., 2016). Joyner and his team follow the infection of malaria strain. cynomolgiin monkeys, taking blood data and other biological information daily.  While the paper discusses possible points of difference between monkeys of varying disease severity, we endeavored to find an automatic way to use these ``clinical and parasitological features" to characterize and predict aspects of the infection, including severity and stage of malaria. We propose to replicate existing analyses and to add new insights via various computational techniques. Machine learning is traditionally used for very large datasets, and thus this thesis, limited to a small dataset by the exorbitant cost of studying monkeys, intends to provide a proof of concept for automatically analyzing these types of data. The flow of computation is as follows: normalization of data, creation of mathematical models, residual calculation, formation of residual matrices, and lastly the generation of regression models. The aforementioned pro-cedure is then applied to shifted data for comparison, using Bayesian optimization. This study therefore provides a comprehensive framework for automatic analysis of medical data, which can be applied to other datasets.},
	Address = {Atlanta, GA},
	Author = {Hexter, Lindsay},
	Date-Added = {2018-07-10 16:44:19 +0000},
	Date-Modified = {2019-05-28 14:06:30 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2018.},
	School = {Emory University},
	Title = {{Monkeying Around: Automatically Analyzing Malaria Infections in Rhesus Macaques}},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/bk128992v},
	Url_Slides = {https://www.slideshare.net/jchoi7s/monkeying-around-automatically-analyzing-malaria-infections-in-rhesus-macaques},
	Year = {2018},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@inproceedings{zhou:18a,
	Abstract = {This paper analyzes arguably the most challenging yet under-explored aspect of resolution tasks such as coreference resolution and entity linking, that is the resolution of plural mentions. Unlike singular mentions each of which represents one entity, plural mentions stand for multiple entities. To tackle this aspect, we take the character identification corpus from the SemEval 2018 shared task that consists of entity annotation for singular mentions, and expand it by adding annotation for plural mentions. We then introduce a novel coreference resolution algorithm that selectively creates clusters to handle both singular and plural mentions, and also a deep learning-based entity linking model that jointly handles both types of mentions through multi-task learning. Adjusted evaluation metrics are proposed for these tasks as well to handle the uniqueness of plural mentions. Our experiments show that the new coreference resolution and entity linking models significantly outperform traditional models designed only for singular mentions. To the best of our knowledge, this is the first time that plural mentions are thoroughly analyzed for these two resolution tasks.},
	Address = {Santa Fe, NM},
	Author = {Zhou, Ethan and Choi, Jinho D.},
	Booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	Date-Added = {2018-05-18 01:20:56 +0000},
	Date-Modified = {2018-08-21 17:17:45 +0000},
	Keywords = {character-mining,emorynlp,selected},
	Pages = {24--34},
	Series = {COLING'18},
	Title = {{They Exist! Introducing Plural Mentions to Coreference Resolution and Entity Linking}},
	Url = {https://coling2018.org/},
	Url_Paper = {http://aclweb.org/anthology/C18-1003.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/they-exist-introducing-plural-mentions-to-coreference-resolution-and-entity-linking},
	Year = {2018},
	Bdsk-Url-1 = {https://coling2018.org/}}

@unpublished{gao:18a,
	Author = {Gao, Shen and Choi, Jinho D.},
	Date-Added = {2018-05-04 14:24:30 +0000},
	Date-Modified = {2018-05-04 14:25:05 +0000},
	Keywords = {emorynlp},
	Note = {Scholarly Inquiry and Research at Emory (SIRE).},
	Organization = {Emory University},
	Title = {{Topic Segmentation in Dialogue}},
	Url = {http://college.emory.edu/undergraduate-research},
	Url_Slides = {https://www.slideshare.net/jchoi7s/topic-segmentation-in-dialogue},
	Year = {2018},
	Bdsk-Url-1 = {http://college.emory.edu/undergraduate-research}}

@inproceedings{choi:18a,
	Abstract = {Character identification is a task of entity linking that finds the global entity of each personal mention in multiparty dialogue. For this task, the first two seasons of the popular TV show Friends are annotated, comprising a total of 448 dialogues, 15,709 mentions, and 401 entities. The personal mentions are detected from nomi-nals referring to certain characters in the show, and the entities are collected from the list of all characters in those two seasons of the show. This task is challenging because it requires the identification of characters that are mentioned but may not be active during the conversation. Among 90+ participants, four of them submitted their system outputs and showed strengths in different aspects about the task. Thorough analyses of the distributed datasets, system outputs , and comparative studies are also provided. To facilitate the momentum, we create an open-source project for this task and publicly release a larger and cleaner dataset, hoping to support researchers for more enhanced modeling.},
	Address = {New Orleans, LA},
	Author = {Choi, Jinho D. and Chen, Henry Y.},
	Booktitle = {Proceedings of the International Workshop on Semantic Evaluation},
	Date-Added = {2018-04-30 15:52:26 +0000},
	Date-Modified = {2018-07-02 16:20:52 +0000},
	Keywords = {emorynlp,character-mining,selected},
	Pages = {57--64},
	Series = {SemEval'18},
	Title = {{SemEval 2018 Task 4: Character Identification on Multiparty Dialogues}},
	Url = {http://alt.qcri.org/semeval2018/},
	Url_Paper = {http://aclweb.org/anthology/S18-1007.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/semeval-2018-task-4-character-identification-on-multiparty-dialogues},
	Year = {2018},
	Bdsk-Url-1 = {http://alt.qcri.org/semeval2018/}}

@techreport{jurczyk:18a,
	Abstract = {This paper gives comprehensive analyses of corpora based on Wikipedia for several tasks in question answering. Four recent corpora are collected, WIKIQA, SELQA, SQUAD, and INFOBOXQA, and first analyzed intrinsically by contextual similarities, question types, and answer categories. These corpora are then analyzed extrinsically by three question answering tasks, answer retrieval, selection, and triggering. An indexing-based method for the creation of a silver-standard dataset for answer retrieval using the entire Wikipedia is also presented. Our analysis shows the uniqueness of these corpora and suggests a better use of them for statistical question answering learning.},
	Author = {Jurczyk, Tomasz and Deshmane, Amit and Choi, Jinho D.},
	Date-Added = {2018-02-23 18:11:43 +0000},
	Date-Modified = {2018-05-04 14:14:53 +0000},
	Institution = {ArXiv},
	Keywords = {emorynlp,selected},
	Number = {1801.02073},
	Title = {Analysis of Wikipedia-based Corpora for Question Answering},
	Url = {https://www.researchgate.net/publication/324941689_Analysis_of_Wikipedia-based_Corpora_for_Question_Answering},
	Year = {2018},
	Bdsk-Url-1 = {https://arxiv.org/abs/1801.02073}}

@inproceedings{ma:18a,
	Abstract = {This paper presents a new corpus and a robust deep learning architecture for a task in reading comprehension, passage completion, on multiparty dialog. Given a dialog in text and a passage containing factual descriptions about the dialog where mentions of the characters are replaced by blanks, the task is to fill the blanks with the most appropriate character names that reflect the contexts in the dialog. Since there is no dataset that challenges the task of passage completion in this genre, we create a corpus by selecting transcripts from a TV show that comprise 1,682 dialogs, generating passages for each dialog through crowdsourcing, and annotating mentions of characters in both the dialog and the passages. Given this dataset, we build a deep neural model that integrates rich feature extraction from convolutional neural networks into sequence modeling in recurrent neural networks, optimized by utterance and dialog level attentions. Our model outperforms the previ- ous state-of-the-art model on this task in a dif- ferent genre using bidirectional LSTM, showing a 13.0+% improvement for longer dialogs. Our analysis shows the effectiveness of the attention mechanisms and suggests a direction to machine comprehension on multiparty dialog.},
	Address = {New Orleans, LA},
	Author = {Ma, Kaixin and Jurczyk, Tomasz and Choi, Jinho D.},
	Booktitle = {Proceedings of the 16th Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	Date-Added = {2018-02-14 21:37:32 +0000},
	Date-Modified = {2018-08-21 19:06:27 +0000},
	Keywords = {emorynlp,selected,character-mining},
	Pages = {2039--2048},
	Series = {NAACL'18},
	Title = {{Challenging Reading Comprehension on Daily Conversation: Passage Completion on Multiparty Dialog}},
	Url = {http://naacl2018.org},
	Url_Paper = {http://aclweb.org/anthology/N18-1185.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/challenging-reading-comprehension-on-daily-conversation-passage-completion-on-multiparty-dialog},
	Year = {2018},
	Bdsk-Url-1 = {http://naacl2018.org}}

@inproceedings{chun:18a,
	Abstract = {This paper presents three treebanks in Korean that consist of dependency trees derived from existing treebanks, the Google UD Treebank, the Penn Korean Treebank, and the KAIST Treebank, and pseudo-annotated by the latest guidelines from the Universal Dependencies (UD) project. The Korean portion of the Google UD Treebank is re-tokenized to match the morpheme-level annotation suggested by the other corpora, and systematically assessed for errors. Phrase structure trees in the Penn Korean Treebank and the KAIST Treebank are automatically converted into dependency trees using head finding rules and linguistic heuristics. Additionally, part-of-speech tags in all treebanks are converted into the UD tagset. A total of 38K+ dependency trees are generated that comprise a coherent set of dependency relations for over a half million tokens. To the best of our knowledge, this is the first time that these Korean corpora are analyzed together and transformed into dependency trees following the latest UD guidelines, version 2.},
	Address = {Miyazaki, Japan},
	Author = {Chun, Jayeol and Han, Na-Rae and Hwang, Jena D. and Choi, Jinho D.},
	Booktitle = {Proceedings of the 11th International Conference on Language Resources and Evaluation},
	Date-Added = {2017-12-13 22:11:04 +0000},
	Date-Modified = {2018-07-02 16:39:15 +0000},
	Keywords = {selected,emorynlp},
	Pages = {2194--2202},
	Series = {LREC'18},
	Title = {{Building Universal Dependency Treebanks in Korean}},
	Url = {http://lrec2018.lrec-conf.org},
	Url_Paper = {http://www.lrec-conf.org/proceedings/lrec2018/pdf/378.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/building-universal-dependency-treebanks-in-korean},
	Year = {2018},
	Bdsk-Url-1 = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/903_Paper.pdf}}

@techreport{chokshi:17a,
	Author = {Chokshi, Falgun and Shin, Bonggun and Lee, Timothy and Lemmon, Andrew and Necessary, Sean and Choi, Jinho D.},
	Date-Added = {2017-12-09 13:36:48 +0000},
	Date-Modified = {2018-02-27 15:55:30 +0000},
	Institution = {bioRxiv},
	Keywords = {selected,emorynlp,medical},
	Number = {173310},
	Title = {{Natural Language Processing for Classification of Acute, Communicable Findings on Unstructured Head CT Reports: Comparison of Neural Network and Non-Neural Machine Learning Techniques}},
	Url = {https://www.biorxiv.org/content/early/2017/08/09/173310},
	Url_Paper = {https://www.biorxiv.org/content/early/2017/08/09/173310.full.pdf},
	Year = {2017},
	Bdsk-Url-1 = {https://www.biorxiv.org/content/early/2017/08/09/173310}}

@inproceedings{choi:09a,
	Abstract = {This short paper describes the use of the linguistic annotation available in parallel PropBanks (Chinese and English) for the enhancement of automatically derived word alignments. Specifically, we suggest ways to refine and expand word alignments for verb-predicates by using predicate-argument structures. Evaluations demonstrate improved alignment accuracies that vary by corpus type.},
	Address = {Suntec, Singapore},
	Author = {Choi, Jinho D. and Palmer, Martha and Xue, Nianwen},
	Booktitle = {Proceedings of the ACL Workshop on Linguistic Annotation},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:13:07 +0000},
	Keywords = {selected},
	Pages = {121--124},
	Series = {LAW'09},
	Title = {{Using Parallel Propbanks to enhance Word-alignments}},
	Url = {http://wmmks.csie.ncku.edu.tw/ACL-IJCNLP-2009/LAW-III},
	Url_Paper = {http://www.aclweb.org/anthology/W09-3020.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/using-parallel-propbanks-to-enhance-wordalignments-110906173},
	Year = {2009},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/W09-3020}}

@inbook{choi:09b,
	Abstract = {We combine transition-based dependency parsing with a high-performing but relatively underexplored machine learning technique, Robust Risk Minimization. During decoding, we judiciously prune the next parsing states using k-best ranking. Moreover, we apply a simple post-processing to ensure robustness. We evaluate our approach on the CoNLL'09 shared task English data and improve transition-based dependency parsing accuracy, reaching a labeled attachment score of 89.28%. We also have observed near quadratic average running time in practice for the algorithm.},
	Author = {Choi, Jinho D. and Nicolov, Nicolas},
	Booktitle = {Recent Advances in Natural Language Processing V},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:31:33 +0000},
	Editor = {Nicolas Nicolov and Galia Angelova and Ruslan Mitkov},
	Keywords = {selected},
	Pages = {205--216},
	Publisher = {John Benjamins},
	Title = {{K-best, Locally Pruned, Transition-based Dependency Parsing Using Robust Risk Minimization}},
	Url = {https://benjamins.com/#catalog/books/cilt.309.16cho},
	Url_Slides = {https://www.slideshare.net/jchoi7s/k-best-locally-pruned-transitionbased-dependency-parsing-using-robust-risk-minimization},
	Year = {2009},
	Bdsk-Url-1 = {http://profs.info.uaic.ro/~promise2010/book/14.html}}

@techreport{choi:09c,
	Abstract = {This report gives guidelines of how to use a Propbank instance editor, Jubilee. Propbank is a cor- pus in which the arguments of each predicate are annotated with their semantic roles in relation to the predicate. In addition to semantic role annotation, Propbank annotation requires the choice of a sense id (also known as a roleset or frameset id) for each predicate. Thus, for each predicate in the Propbank, there exists a corresponding frameset file that shows the predicate argument structure of all senses related to the predicate. Jubilee facilitates the annotation process by displaying to the annotator several resources of syntactic and semantic information simultaneously: firstly, the syntactic structure of a sentence is displayed in the main frame using the Penn Treebank style of phrase structure; secondly, the available senses with their corresponding argument structures are displayed in another frame; thirdly, all available Propbank arguments including modifiers (e.g., ARG0, ARGM-TMP) are displayed for the annotator's choice; finally, example annotations of each sense of the predicate are available to the annotator for viewing whenever desired. The easy use of each resource allows the annotator to quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation. Jubilee has been successfully adopted to Propbank projects in several organizations such as the University of Colorado at Boulder, the University of Illinois at Urbana-Champaign, and Brandeis University. The tool runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.
},
	Author = {Choi, Jinho D. and Bonial, Claire and Palmer, Martha},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:30:05 +0000},
	Institution = {University of Colorado Boulder},
	Keywords = {selected},
	Number = {02-09},
	Title = {{Jubilee: Propbank Instance Editor Guideline (Version 2.1)}},
	Url_Paper = {https://www.researchgate.net/publication/228684269_Jubilee_Propbank_Instance_Editor_Guideline_Version_21},
	Year = {2009},
	Bdsk-Url-1 = {http://www.mathcs.emory.edu/~choi/doc/jubilee-2009.pdf}}

@techreport{choi:09d,
	Abstract = {This report gives guidelines of how to use a Propbank frameset editor, Cornerstone. Propbank is a corpus where the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense id (also known as a frameset or roleset id) for each predicate. Therefore, for each predicate in the Propbank, there exists a corresponding frameset file that shows the predicate argument structure of all senses related to the predicate. Since most Propbank annotations are based on the predicate argument structure defined in the frameset files, it is important to keep the files consistent, simple to read as well as easy to update. Up to this point, all frameset files are written in xml format, which provides a well-organized hierarchical structure, but is difficult to edit without making mistakes. Therefore, it is necessary to develop a user-friendly editor such as Cornerstone, that is specifically customized to view, create and edit frameset files. Cornerstone runs platform independently, is light enough to run as an X11 application and supports multiple lan- guages such as Arabic, Chinese, English, Hindi and Korean.},
	Author = {Choi, Jinho D. and Bonial, Claire and Palmer, Martha},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:30:38 +0000},
	Institution = {University of Colorado Boulder},
	Keywords = {selected},
	Number = {01-09},
	Title = {{Cornerstone: Propbank Frameset Editor Guideline (Version 1.3)}},
	Url_Paper = {https://www.researchgate.net/publication/228469837_Cornerstone_Propbank_Frameset_Editor_Guideline_Version_13},
	Year = {2009},
	Bdsk-Url-1 = {http://www.mathcs.emory.edu/~choi/doc/cornerstone-2009.pdf}}

@techreport{bonial:10a,
	Author = {Bonial, Claire and Babko-Malaya, Olga and Choi, Jinho D. and Hwang, Jena and Palmer, Martha},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:02:22 +0000},
	Institution = {University of Colorado Boulder},
	Keywords = {selected},
	Title = {{PropBank Annotation Guidelines (Version 3.0)}},
	Url_Paper = {doc/cu-2010-bonial.pdf},
	Year = {2010},
	Bdsk-Url-1 = {doc/cu-2010-bonial.pdf}}

@inbook{choi:10a,
	Abstract = {In this paper, we introduce a way of improving the parsing accuracy of a transition-based dependency parsing model by using k-best ranking. Our approach uses a broader search space than beam search, yet keeps the parsing complexity near a quadratic average running time. In addition, we take a simple post-processing step to ensure the parsing output is a connected dependency tree. As an oracle, we use a high-performing but relatively underexplored machine learning algorithm, Robust Risk Minimization, which gives a higher parsing accuracy than the Perceptron algorithm in the experiments. We also use an automatic feature reduction technique that reduces the feature space by about 49% without compromising the parsing accuracy. We evaluate our approach on the CoNLL'09 shared task English data and improve the transition-based dependency parsing accuracy, showing a 0.64% higher accuracy than the best transition-based CoNLL'09 system.},
	Author = {Choi, Jinho D. and Nicolov, Nicolas},
	Booktitle = {Multilinguality and Interoperability in Language Processing with Emphasis on Romanian},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:02:26 +0000},
	Editor = {Dan Tufis and Corina Forascu},
	Keywords = {selected},
	Pages = {288--302},
	Publisher = {Romanian Academy Publishing House},
	Title = {{K-best, Transition-based Dependency Parsing Using Robust Risk Minimization and Automatic Feature Reduction}},
	Url = {http://profs.info.uaic.ro/~promise2010/book/14.html},
	Year = {2010},
	Bdsk-Url-1 = {http://profs.info.uaic.ro/~promise2010/book/14.html}}

@inproceedings{choi:10b,
	Abstract = {This paper suggests a robust way of converting constituent-based trees in the Penn Treebank style into dependency trees for several different English corpora. For English, there already exist conversion tools. However, these tools are often customized enough for a specific corpus that they do not nec- essarily work as well when applied to different corpora involving newly introduced POS-tags or annotation schemes. The desire to improve conversion portability motivated us to build a new conversion tool that would produce more robust results across different corpora. In particular, we have modi- fied the treatment of head-percolation rules, function tags, coordination, gapping, and empty category mappings. We compare our method with the LTH conversion tool used for the CoNLL'07-09 shared tasks. For our experiments, we use 6 different English corpora from OntoNotes release 4.0. To demonstrate the impact our approach has on parsing, we train and test two state-of-the-art dependency parsers, MaltParser and MSTParser, and our own parser, ClearParser, using converted output from both the LTH tool and our method. Our results show that our method removes certain unnecessary non- projective dependencies and generates fewer unclassified dependencies. All three parsers give higher parsing accuracies on average across these corpora using data generated by our method; especially on semantic dependencies.},
	Address = {Tartu, Estonia},
	Author = {Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the 9th International Workshop on Treebanks and Linguistic Theories},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:19:22 +0000},
	Keywords = {selected},
	Pages = {55--66},
	Series = {TLT'10},
	Title = {{Robust Constituent-to-Dependency Conversion for English}},
	Url = {http://math.ut.ee/tlt9/},
	Url_Paper = {http://dspace.utlib.ee/dspace/bitstream/10062/15934/1/tlt9_submission_3.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/robust-constituenttodependency-conversion-for-english},
	Year = {2010},
	Bdsk-Url-1 = {http://dspace.utlib.ee/dspace/bitstream/10062/15934/1/tlt9_submission_3.pdf}}

@inproceedings{choi:10c,
	Abstract = {This paper gives guidelines of how to annotate Propbank instances using a dedicated editor, Jubilee. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Jubilee facilitates this annotation process by displaying several resources of syntactic and semantic information simultaneously: the syntactic structure of a sentence is displayed in the main frame, the available senses with their corresponding argument structures are displayed in another frame, all available Propbank arguments are displayed for the annotators choice, and example annotations of each sense of the predicate are available to the annotator for viewing. Easy access to each of these resources allows the annotator to quickly absorb and apply the necessary syntactic and semantic information pertinent to each predicate for consistent and efficient annotation. Jubilee has been successfully adapted to many Propbank projects in several universities. The tool runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.},
	Address = {Valletta, Malta},
	Author = {Choi, Jinho D. and Bonial, Claire and Palmer, Martha},
	Booktitle = {Proceedings of the 7th International Conference on Language Resources and Evaluation},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:18:19 +0000},
	Keywords = {selected},
	Pages = {1871--1875},
	Series = {LREC'10},
	Title = {{Propbank Instance Annotation Guidelines Using a Dedicated Editor, Jubilee}},
	Url = {http://www.lrec-conf.org/lrec2010/},
	Url_Paper = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/903_Paper.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/propbank-instance-annotation-guidelines-using-a-dedicated-editor-jubilee},
	Year = {2010},
	Bdsk-Url-1 = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/903_Paper.pdf}}

@inproceedings{choi:10d,
	Abstract = {This paper gives guidelines of how to create and update Propbank frameset files using a dedicated editor, Cornerstone. Propbank is a corpus in which the arguments of each verb predicate are annotated with their semantic roles in relation to the predicate. Propbank annotation also requires the choice of a sense ID for each predicate. Thus, for each predicate in Propbank, there exists a corresponding frameset file showing the expected predicate argument structure of each sense related to the predicate. Since most Propbank annotations are based on the predicate argument structure defined in the frameset files, it is important to keep the files consistent, simple to read as well as easy to update. The frameset files are written in XML, which can be difficult to edit when using a simple text editor. Therefore, it is helpful to develop a user-friendly editor such as Cornerstone, specifically customized to create and edit frameset files. Cornerstone runs platform independently, is light enough to run as an X11 application and supports multiple languages such as Arabic, Chinese, English, Hindi and Korean.},
	Address = {Valletta, Malta},
	Author = {Choi, Jinho D. and Bonial, Claire and Palmer, Martha},
	Booktitle = {Proceedings of the 7th International Conference on Language Resources and Evaluation},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:17:45 +0000},
	Keywords = {selected},
	Pages = {3650--3653},
	Series = {LREC'10},
	Title = {{Propbank Frameset Annotation Guidelines Using a Dedicated Editor, Cornerstone}},
	Url = {http://www.lrec-conf.org/lrec2010/},
	Url_Paper = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/73_Paper.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/propbank-frameset-annotation-guidelines-using-a-dedicated-editor-cornerstone},
	Year = {2010},
	Bdsk-Url-1 = {http://www.lrec-conf.org/proceedings/lrec2010/pdf/903_Paper.pdf}}

@inproceedings{choi:10e,
	Abstract = {This paper describes the retrieval of correct semantic boundaries for predicate-argument structures annotated by dependency structure. Unlike phrase structure, in which arguments are annotated at the phrase level, dependency structure does not have phrases so the argument labels are associated with head words instead: the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. However, at least in English, retrieving such subtrees does not always guarantee retrieval of the correct phrase boundaries. In this paper, we present heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries, from dependency trees. By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries. Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation.},
	Address = {Uppsala, Sweden},
	Author = {Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the ACL Workshop on Linguistic Annotation},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:16:55 +0000},
	Keywords = {selected},
	Pages = {91--99},
	Series = {LAW'10},
	Title = {{Retrieving Correct Semantic Boundaries in Dependency Structure}},
	Url = {http://www.cs.brandeis.edu/~clp/LAW4/The_LAW_IV.html},
	Url_Paper = {http://aclweb.org/anthology/W10-1811.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/retrieving-correct-semantic-boundaries-in-dependency-structure},
	Year = {2010},
	Bdsk-Url-1 = {http://aclweb.org/anthology/W10-1811}}

@inproceedings{wu:10a,
	Abstract = {This paper suggests a method for detecting cross-lingual semantic similarity using parallel PropBanks. We begin by improving word alignments for verb predicates gener- ated by GIZA++ by using information available in parallel PropBanks. We applied the Kuhn-Munkres method to measure predicate-argument matching and improved verb predicate alignments by an F-score of 12.6%. Using the enhanced word alignments we checked the set of target verbs aligned to a specific source verb for semantic consistency. For a set of English verbs aligned to a Chinese verb, we checked if the English verbs belong to the same semantic class using an existing lexi- cal database, WordNet. For a set of Chinese verbs aligned to an English verb we manually checked semantic similarity between the Chinese verbs within a set. Our results show that the verb sets we generated have a high correla- tion with semantic classes. This could potentially lead to an automatic technique for generating semantic classes for verbs.},
	Address = {Denver, CO},
	Author = {Wu, Shumin and Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the 9th Conference of the Association for Machine Translation in the Americas},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:14:50 +0000},
	Keywords = {selected},
	Series = {AMTA'10},
	Title = {{Detecting Cross-lingual Semantic Similarity Using Parallel PropBanks}},
	Url = {https://amta2010.amtaweb.org},
	Url_Paper = {http://www.mt-archive.info/AMTA-2010-Wu.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/detecting-crosslingual-semantic-similarity-using-parallel-propbanks-110906426},
	Year = {2010},
	Bdsk-Url-1 = {http://amta2010.amtaweb.org/AMTA/papers/2-15-WuChoiPalmer.pdf}}

@inproceedings{choi:11a,
	Abstract = {This paper gives two contributions to dependency parsing in Korean. First, we build a Korean dependency Treebank from an existing constituent Treebank. For a morphologically rich language like Korean, dependency parsing shows some advantages over constituent parsing. Since there is not much training data available, we automatically generate dependency trees by applying head-percolation rules and heuristics to the constituent trees. Second, we show how to extract useful features for dependency parsing from rich morphology in Korean. Once we build the dependency Treebank, any statistical parsing approach can be applied. The challenging part is how to extract features from tokens consisting of multiple morphemes. We suggest a way of selecting important morphemes and use only these as features to avoid sparsity. Our parsing approach is evaluated on three different genres using both gold-standard and automatic morphological analysis. We also test the impact of fine vs. coarse-grained morphologies on dependency parsing. With automatic morphological analysis, we achieve labeled attachment scores of 80%+. To the best of our knowledge, this is the first time that Korean dependency parsing has been evaluated on labeled edges with such a large variety of data.
},
	Address = {Dublin, Ireland},
	Author = {Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the IWPT Workshop on Statistical Parsing of Morphologically Rich Languages},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:22:54 +0000},
	Keywords = {selected},
	Pages = {1--11},
	Series = {SPMRL'11},
	Title = {{Statistical Dependency Parsing in Korean: From Corpus Generation To Automatic Parsing}},
	Url = {https://sites.google.com/site/spmrl2011/},
	Url_Paper = {http://aclweb.org/anthology/W11-3801.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/statistical-dependency-parsing-in-korean-from-corpus-generation-to-automatic-parsing},
	Year = {2011},
	Bdsk-Url-1 = {http://aclweb.org/anthology/W11-3801}}

@inproceedings{choi:11b,
	Abstract = {This paper suggests two ways of improving semantic role labeling (SRL). First, we introduce a novel transition-based SRL algorithm that gives a quite different approach to SRL. Our algorithm is inspired by shift-reduce pars- ing and brings the advantages of the transition-based approach to SRL. Second, we present a self-learning clustering technique that effec- tively improves labeling accuracy in the test domain. For better generalization of the sta- tistical models, we cluster verb predicates by comparing their predicate argument structures and apply the clustering information to the final labeling decisions. All approaches are evaluated on the CoNLL'09 English data. The new algorithm shows comparable results to another state-of-the-art system. The clustering technique improves labeling accuracy for both in-domain and out-of-domain tasks.},
	Address = {Portland, OR},
	Author = {Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the ACL Workshop on Relational Models of Semantics},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:22:29 +0000},
	Keywords = {selected},
	Pages = {37--45},
	Series = {RELMS'11},
	Title = {{Transition-based Semantic Role Labeling Using Predicate Argument Clustering}},
	Url = {https://sites.google.com/site/relms2011/},
	Url_Paper = {http://aclweb.org/anthology/W11-0906.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/transitionbased-semantic-role-labeling-using-predicate-argument-clustering},
	Year = {2011},
	Bdsk-Url-1 = {http://aclweb.org/anthology/W11-0906}}

@inproceedings{choi:11c,
	Abstract = {This paper suggests two ways of improving transition-based, non-projective dependency parsing. First, we add a transition to an existing non-projective parsing algorithm, so it can perform either projective or non-projective parsing as needed. Second, we present a boot- strapping technique that narrows down discrepancies between gold-standard and automatic parses used as features. The new addition to the algorithm shows a clear advantage in parsing speed. The bootstrapping technique gives a significant improvement to parsing accuracy, showing near state-of-the- art performance with respect to other parsing approaches evaluated on the same data set.},
	Address = {Portland, OR},
	Author = {Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:22:02 +0000},
	Keywords = {selected},
	Pages = {687--692},
	Series = {ACL'11},
	Title = {{Getting the Most out of Transition-based Dependency Parsing}},
	Url = {http://mirror.aclweb.org/acl2011/},
	Url_Paper = {http://aclweb.org/anthology/P11-2121.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/getting-the-most-out-of-transitionbased-dependency-parsing},
	Year = {2011},
	Bdsk-Url-1 = {http://aclweb.org/anthology/P11-2121}}

@inproceedings{vaidya:11a,
	Abstract = {This paper makes two contributions. First, we describe the Hindi Proposition Bank that contains annotations of predicate argument structures of verb predicates. Unlike PropBanks in most other languages, the Hind PropBank is annotated on top of dependency structure, the Hindi Dependency Treebank. We explore the similarities between dependency and predicate argument structures, so the PropBank annotation can be faster and more accurate. Second, we present a probabilistic rule-based system that maps syntactic dependents to semantic arguments. With simple rules, we classify about 47% of the entire PropBank arguments with over 90% confidence. These preliminary results are promising; they show how well these two frameworks are correlated. This can also be used to speed up our annotations.},
	Address = {Portland, OR},
	Author = {Vaidya, Ashwini and Choi, Jinho D. and Palmer, Martha and Narasimhan, Bhuvana},
	Booktitle = {Proceedings of the ACL Workshop on Linguistic Annotation},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:02:22 +0000},
	Keywords = {selected},
	Pages = {21--29},
	Series = {LAW'11},
	Title = {{Analysis of the Hindi Proposition Bank using Dependency Structure}},
	Url = {http://cemantix.org/workshop/law-v/},
	Url_Paper = {http://aclweb.org/anthology/W11-0403.pdf},
	Year = {2011},
	Bdsk-Url-1 = {http://aclweb.org/anthology/W11-0403}}

@techreport{choi:12a,
	Author = {Choi, Jinho D. and Palmer, Martha},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:27:26 +0000},
	Institution = {University of Colorado Boulder},
	Keywords = {selected},
	Number = {01-12},
	Title = {{Guidelines for the Clear Style Constituent to Dependency Conversion}},
	Url_Paper = {https://www.researchgate.net/publication/324940566_Guidelines_for_the_CLEAR_Style_Constituent_to_Dependency_Conversion},
	Year = {2012},
	Bdsk-Url-1 = {http://www.mathcs.emory.edu/~choi/doc/clear-dependency-2012.pdf}}

@phdthesis{choi:12b,
	Abstract = {This thesis focuses on the optimization of NLP components for robustness and scalability. Three kinds of nlp components are used for our experiments, a part-of-speech tagger, a dependency parser, and a semantic role labeler. For part-of-speech tagging, dynamic model selection is introduced. Our dynamic model selection approach builds two models, domain-specific and generalized models, and selects one of them during decoding by comparing similarities between lexical items used for building these models and input sentences. As a result, it gives robust tagging accuracy across corpora and shows fast tagging speed. For dependency parsing, a new transition-based parsing algorithm and a bootstrapping technique are introduced. Our parsing algorithm learns both projective and non-projective transitions so it can generate both projective and non-projective dependency trees yet shows linear time parsing speed on average. Our bootstrapping technique bootstraps parse information used as features for transition-based parsing, and shows significant improvement for parsing accuracy. For semantic role labeling, a conditional higher-order argument pruning algorithm is introduced. A higher-order pruning algorithm improves the coverage of argument candidates and shows improvement on the overall F1-score. The conditional higher-order pruning algorithm also noticeably reduces average labeling complexity with minimal reduction in F1-score.

For all experiments, two sets of training data are used; one is from the Wall Street Journal corpus, and the other is from the OntoNotes corpora. All components are evaluated on 9 different genres, which are grouped separately for in-genre and out-of-genre experiments. Our experiments show that our approach gives higher accuracies compared to other state-of-the-art nlp components, and runs fast, taking about 3-4 milliseconds per sentence for processing all three components. All components are publicly available as an open source project, called ClearNLP. We believe that this project is beneficial for many nlp tasks that need to process large-scale heterogeneous data.},
	Author = {Choi, Jinho D.},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:28:09 +0000},
	Keywords = {selected},
	Organization = {Department of Computer Science and Institute of Cognitive Science},
	School = {University of Colorado Boulder},
	Title = {{Optimization of Natural Language Processing Components for Robustness and Scalability}},
	Url_Paper = {https://www.researchgate.net/publication/324941325_Optimization_of_Natural_Language_Processing_Components_for_Robustness_and_Scalability},
	Url_Slides = {https://www.slideshare.net/jchoi7s/optimization-of-nlp-components-for-robustness-and-scalability},
	Year = {2012},
	Bdsk-Url-1 = {http://www.mathcs.emory.edu/~choi/doc/thesis-2012.pdf}}

@inproceedings{choi:12c,
	Abstract = {This paper presents a novel way of improving POS tagging on heterogeneous data. First, two separate models are trained (generalized and domain-specific) from the same data set by controlling lexical items with different document frequencies. During decoding, one of the models is selected dynamically given the cosine similarity between each sentence and the training data. This dynamic model selec- tion approach, coupled with a one-pass, left-to-right POS tagging algorithm, is evaluated on corpora from seven different genres. Even with this simple tagging algorithm, our system shows comparable results against other state-of-the-art systems, and gives higher accuracies when evaluated on a mixture of the data. Furthermore, our system is able to tag about 32K tokens per second. We believe that this model selection approach can be applied to more sophisticated tagging algorithms and improve their robustness even further.},
	Address = {Jeju Island, Korea},
	Author = {Choi, Jinho D. and Palmer, Martha},
	Booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:24:17 +0000},
	Keywords = {selected},
	Pages = {363--367},
	Series = {ACL'12},
	Title = {{Fast and Robust Part-of-Speech Tagging Using Dynamic Model Selection}},
	Url = {https://www.aclweb.org/mirror/acl2012/},
	Url_Paper = {http://www.aclweb.org/anthology/P12-2071.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/acl12-poster},
	Year = {2012},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/P12-2071}}

@inproceedings{vaidya:12a,
	Abstract = {This paper examines both linguistic behavior and practical implications of empty argument insertion in the Hindi PropBank. The Hindi PropBank is annotated on the Hindi Dependency Treebank, which contains some empty categories but rarely the empty arguments of verbs. In this paper, we analyze four kinds of empty arguments, *PRO*, *REL*, *GAP*, *pro*, and suggest effective ways of annotating these arguments. Empty arguments such as *PRO* and *REL* can be inserted deterministically; we present linguistically motivated rules that automatically insert these arguments with high accuracy. On the other hand, it is difficult to find deterministic rules to insert *GAP* and *pro*; for these arguments, we introduce a new annotation scheme that concurrently handles both semantic role labeling and empty category insertion, producing fast and high quality annotation. In addition, we present algorithms for finding antecedents of *REL* and *PRO*, and discuss why finding antecedents for some types of *PRO* is difficult.},
	Address = {Istanbul, Turkey},
	Author = {Vaidya, Ashwini and Choi, Jinho D. and Palmer, Martha and Narasimhan, Bhuvana},
	Booktitle = {Proceedings of the 8th International Conference on Language Resources and Evaluation},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:02:22 +0000},
	Keywords = {selected},
	Pages = {1522--1526},
	Series = {LREC'12},
	Title = {{Empty Argument Insertion in the Hindi PropBank}},
	Url = {http://www.lrec-conf.org/lrec2012/},
	Url_Paper = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/442_Paper.pdf},
	Year = {2012},
	Bdsk-Url-1 = {http://www.lrec-conf.org/proceedings/lrec2012/pdf/442_Paper.pdf}}

@article{verspoor:12a,
	Abstract = {Background
We introduce the linguistic annotation of a corpus of 97 full-text biomedical publications, known as the Colorado Richly Annotated Full Text (CRAFT) corpus. We further assess the performance of existing tools for performing sentence splitting, tokenization, syntactic parsing, and named entity recognition on this corpus.

Results
Many biomedical natural language processing systems demonstrated large differences between their previously published results and their performance on the CRAFT corpus when tested with the publicly available models or rule sets. Trainable systems differed widely with respect to their ability to build high-performing models based on this data.

Conclusions
The finding that some systems were able to train high-performing models based on this corpus is additional evidence, beyond high inter-annotator agreement, that the quality of the CRAFT corpus is high. The overall poor performance of various systems indicates that considerable work needs to be done to enable natural language processing systems to work well when the input is full-text journal articles. The CRAFT corpus provides a valuable resource to the biomedical natural language processing community for evaluation and training of new models for biomedical full text publications.},
	Author = {Verspoor, Karin and Cohen, Kevin B. and Lanfranchi, Arrick and Warner, Colin and Johnson, Helen L. and Roeder, Christophe and Choi, Jinho D. and Funk, Christopher and Malenkiy, Yuriy and Eckert, Miriam and Xue, Nianwen and Baumgartner Jr., William A. and Bada, Mike and Palmer, Martha and Hunter, Larry E.},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-02-27 15:57:23 +0000},
	Journal = {BMC Bioinformatics},
	Keywords = {selected,medical},
	Number = {207},
	Title = {{A corpus of full-text journal articles is a robust evaluation tool for revealing differences in performance of biomedical natural language processing tools}},
	Url = {http://www.biomedcentral.com/1471-2105/13/207},
	Volume = {13},
	Year = {2012},
	Bdsk-Url-1 = {http://www.biomedcentral.com/1471-2105/13/207}}

@article{albright:13a,
	Abstract = {Objective
To create annotated clinical narratives with layers of syntactic and semantic labels to facilitate advances in clinical natural language processing (NLP).
To develop NLP algorithms and open source components.

Methods
Manual annotation of a clinical narrative corpus of 127 606 tokens following the Treebank schema for syntactic information, PropBank schema for predicate-argument structures, and the Unified Medical Language System (UMLS) schema for semantic information. NLP components were developed.

Results
The final corpus consists of 13 091 sentences containing 1772 distinct predicate lemmas. Of the 766 newly created PropBank frames, 74 are verbs. There are 28 539 named entity (NE) annotations spread over 15 UMLS semantic groups, one UMLS semantic type, and the Person semantic category. The most frequent annotations belong to the UMLS semantic groups of Procedures (15.71%), Disorders (14.74%), Concepts and Ideas (15.10%), Anatomy (12.80%), Chemicals and Drugs (7.49%), and the UMLS semantic type of Sign or Symptom (12.46%). Inter-annotator agreement results: Treebank (0.926), PropBank (0.891--0.931), NE (0.697--0.750). The part-of-speech tagger, constituency parser, dependency parser, and semantic role labeler are built from the corpus and released open source. A significant limitation uncovered by this project is the need for the NLP community to develop a widely agreed-upon schema for the annotation of clinical concepts and their relations.

Conclusions
This project takes a foundational step towards bringing the field of clinical NLP up to par with NLP in the general domain. The corpus creation and NLP components provide a resource for research and application development that would have been previously impossible.},
	Author = {Albright, Daniel and Lanfranchi, Arrick and Fredriksen, Anwen and Styler, William F. and Warner, Colin and Hwang, Jena D. and Choi, Jinho D. and Dligach, Dmitriy and Nielsen, Rodney D. and Martin, James and Ward, Wayne and Palmer, Martha and Savova, Guergana K.},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:36:23 +0000},
	Journal = {Journal of the American Medical Informatics Association},
	Keywords = {selected,medical},
	Number = {5},
	Pages = {922--930},
	Title = {{Towards comprehensive syntactic and semantic annotations of the clinical narrative}},
	Url_Paper = {http://jamia.oxfordjournals.org/content/jaminfo/20/5/922.full.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/towards-comprehensive-syntactic-and-semantic-annotations-of-the-clinical-narrative},
	Volume = {20},
	Year = {2013},
	Bdsk-Url-1 = {http://dx.doi.org/10.1136/amiajnl-2012-001317}}

@techreport{choi:13a,
	Abstract = {This document gives a brief description of Korean data prepared for the SPMRL 2013 shared task (Seddah et al., 2013). A total of 27,363 sentences with 350,090 tokens are used for the shared task. All constituent trees are collected from the KAIST Treebank and transformed to the Penn Treebank style. All dependency trees are converted from the transformed constituent trees us- ing heuristics and labeling rules designed specifically for the KAIST Treebank. In addition to the gold-standard morphological analysis provided by the KAIST Treebank, two sets of automatic morphological analy- sis are provided for the shared task, one is generated by the HanNanum morphologi- cal analyzer, and the other is generated by the Sejong morphological analyzer.},
	Author = {Choi, Jinho D.},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:02:22 +0000},
	Institution = {ArXiv},
	Keywords = {selected},
	Number = {1309.1649},
	Title = {{Preparing Korean Data for the Shared Task on Parsing Morphologically Rich Languages}},
	Url = {http://arxiv.org/abs/1309.1649},
	Year = {2013},
	Bdsk-Url-1 = {http://arxiv.org/abs/1309.1649}}

@inproceedings{choi:13b,
	Abstract = {We present a novel approach, called selectional branching, which uses confidence estimates to decide when to employ a beam, providing the accuracy of beam search at speeds close to a greedy transition-based dependency parsing approach. Selectional branching is guaranteed to perform a fewer number of transitions than beam search yet performs as accurately. We also present a new transition-based dependency parsing algorithm that gives a complexity of O(n) for projective parsing and an expected linear time speed for non-projective parsing. With the standard setup, our parser shows an unlabeled attachment score of 92.96% and a parsing speed of 9 milliseconds per sentence, which is faster and more accurate than the current state-of-the-art transition- based parser that uses beam search.},
	Address = {Sofia, Bulgaria},
	Author = {Choi, Jinho D. and McCallum, Andrew},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:37:26 +0000},
	Keywords = {selected},
	Pages = {1052--1062},
	Series = {ACL'13},
	Title = {{Transition-based Dependency Parsing with Selectional Branching}},
	Url = {http://mirror.aclweb.org/acl2013/},
	Url_Paper = {http://www.aclweb.org/anthology/P13-1104.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/transitionbased-dependency-parsing-with-selectional-branching-27507631},
	Year = {2013},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/P13-1104}}

@inproceedings{seddah:13a,
	Abstract = {This paper reports on the first shared task on statistical parsing of morphologically rich lan- guages (MRLs). The task features data sets from nine languages, each available both in constituency and dependency annotation. We report on the preparation of the data sets, on the proposed parsing scenarios, and on the evaluation metrics for parsing MRLs given different representation types. We present and analyze parsing results obtained by the task participants, and then provide an analysis and comparison of the parsers across languages and frameworks, reported for gold input as well as more realistic parsing scenarios.
},
	Address = {Seattle, WA},
	Author = {Seddah, Djam\'{e} and Tsarfaty, Reut and K\"{u}bler, Sandra and Candito, Marie and Choi, Jinho D. and Farkas, Rich\'{a}rd and Foster, Jennifer and Goenaga, Iakes and Gojenola Galletebeitia, Koldo and Goldberg, Yoav and Green, Spence and Habash, Nizar and Kuhlmann, Marco and Maier, Wolfgang and Nivre, Joakim and Przepi\'{o}rkowski, Adam and Roth, Ryan and Seeker, Wolfgang and Versley, Yannick and Vincze, Veronika and Woli\'{n}ski, Marcin and Wr\'{o}blewska, Alina and de la Clergerie, Eric Villemonte},
	Booktitle = {Proceedings of the EMNLP Workshop on Statistical Parsing of Morphologically-Rich Languages},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:01:38 +0000},
	Keywords = {selected},
	Pages = {146--182},
	Series = {SPMRL'13},
	Title = {{Overview of the SPMRL 2013 Shared Task: Cross-Framework Evaluation of Parsing Morphologically Rich Languages}},
	Url = {https://sites.google.com/site/spmrl2013/},
	Url_Paper = {http://www.aclweb.org/anthology/W13-4917.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/W13-4917}}

@inproceedings{singh:13a,
	Abstract = {We employ universal schema for slot filling and cold start. In universal schema, we allow each surface pattern from raw text, and each type defined in ontology, i.e. TACKBP slots to represent relations. And we use matrix factorization to discover implications among surface patterns and target slots. First, we identify mentions of entities from the whole text corpus and extract relations between entity pairs to construct a knowledge base. Finally, we query this knowledge base to produce our sub- missions for slot filling and cold start.},
	Address = {Gaithersburg, MD},
	Author = {Singh, Sameer and Yao, Limin and Belanger, David and Kobren, Ari and Anzaroot, Sam and Wick, Mike and Passos, Alexandre and Pandya, Harshal and Choi, Jinho D. and Martin, Brian and McCallum, Andrew},
	Booktitle = {Proceedings of the Text Analysis Conference},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:01:38 +0000},
	Keywords = {selected},
	Series = {TAC'13},
	Title = {{Universal Schema for Slot Filling and Cold Start: UMass IESL at TACKBP 2013}},
	Url_Paper = {http://www.nist.gov/tac/publications/2013/participant.papers/UMass_IESL.TAC2013.proceedings.pdf},
	Year = {2013},
	Bdsk-Url-1 = {http://www.nist.gov/tac/publications/2013/participant.papers/UMass_IESL.TAC2013.proceedings.pdf}}

@inproceedings{zheng:13a,
	Abstract = {Coreference resolution systems can benefit greatly from inclusion of global context, and a number of recent approaches have demonstrated improvements when precomputing an alignment to external knowledge sources. However, since alignment itself is a challenging task and is often noisy, existing systems either align conservatively, resulting in very few links, or combine the attributes of multiple candidates, leading to a conflation of entities. Our approach instead maintains ranked lists of candidate entities that are dynamically merged and reranked during inference. Further, we incorporate a large set of surface string variations for each entity by using anchor texts from the web that link to the entity. These forms of global context enable our system to outperform a competitive baseline without a knowledge base by 1.09 B3 F1 points, and a state-of- the-art system by 0.41 points on the ACE 2004 data.},
	Address = {Sofia, Bulgaria},
	Author = {Zheng, Jiaping and Vilnis, Luke and Singh, Sameer and Choi, Jinho D. and McCallum, Andrew},
	Booktitle = {Proceedings of the 17th Conference on Computational Natural Language Learning},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:37:48 +0000},
	Keywords = {selected},
	Pages = {153--162},
	Series = {CoNLL'13},
	Title = {{Dynamic Knowledge-Base Alignment for Coreference Resolution}},
	Url_Paper = {http://aclweb.org/anthology/W13-3517.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/conll13-slides},
	Year = {2013},
	Bdsk-Url-1 = {http://aclweb.org/anthology/W13-3517}}

@inproceedings{artz:14a,
	Abstract = {One of the challenges analyzing medical conditions in unstructured data is in determining whether they are reported as indications or side effects. Classifying different types of medical conditions enables a deeper understanding of disease manifestation and treatment risk, helping further research in this field. In our study, we collect medical forum posts, annotate different types of medical conditions, and classify medical conditions into indications and side effects using natural language processing and machine learning techniques.},
	Address = {Washington, DC},
	Author = {Artz, Nathan and Choi, Jinho D. and Sawyer, James},
	Booktitle = {Proceedings of the American Medical Informatics Association Annual Symposium},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2018-08-21 18:39:07 +0000},
	Keywords = {selected,medical},
	Pages = {1315},
	Series = {AMIA'14},
	Title = {{Finding Different Types of Medical Conditions: From Data Generation to Automatic Classification}},
	Url = {https://knowledge.amia.org/56638-amia-1.1540970/t-005-1.1543914/f-005-1.1543915/a-247-1.1544930/an-247-1.1544931?qr=1},
	Url_Slides = {https://www.slideshare.net/jchoi7s/finding-different-types-of-medical-conditions-from-data-generation-to-automatic-classification},
	Year = {2014},
	Bdsk-Url-1 = {http://knowledge.amia.org/56638-amia-1.1540970/t-005-1.1543914/f-005-1.1543915/a-247-1.1544930/an-247-1.1544931?qr=1}}

@techreport{ashwini:14a,
	Abstract = {We present a novel approach for recognizing what we call targetable named entities; that is, named entities in a targeted set (e.g, movies, books, TV shows). Unlike many other NER systems that need to retrain their statistical models as new entities arrive, our approach does not require such retraining, which makes it more adaptable for types of entities that are frequently updated. For this preliminary study, we focus on one entity type, movie title, using data collected from Twitter. Our system is tested on two evaluation sets, one including only entities corresponding to movies in our training set, and the other excluding any of those entities. Our final model shows F1-scores of 76.19% and 78.70% on these evaluation sets, which gives strong evidence that our approach is completely unbiased to any par- ticular set of entities found during training.},
	Author = {Ashwini, Sandeep and Choi, Jinho D.},
	Date-Added = {2017-08-31 03:01:32 +0000},
	Date-Modified = {2017-08-31 03:02:43 +0000},
	Institution = {ArXiv},
	Keywords = {selected},
	Number = {1408.0782},
	Title = {{Targetable Named Entity Recognition in Social Media}},
	Url = {https://arxiv.org/abs/1408.0782},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1408.0782}}

@inproceedings{zahiri:18a,
	Abstract = {This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an abso- lute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers.},
	Address = {New Orleans, LA},
	Author = {Zahiri, Sayyed and Choi, Jinho D.},
	Booktitle = {Proceedings of the AAAI Workshop on Affective Content Analysis},
	Date-Added = {2017-08-30 22:27:32 +0000},
	Date-Modified = {2019-07-08 11:28:01 -0400},
	Keywords = {emorynlp,selected,character-mining},
	Pages = {44--51},
	Series = {AFFCON'18},
	Title = {{Emotion Detection on TV Show Transcripts with Sequence-based Convolutional Neural Networks}},
	Url = {https://sites.google.com/view/affcon18},
	Url_Paper = {https://aaai.org/ocs/index.php/WS/AAAIW18/paper/download/16434/15540},
	Url_Slides = {https://www.slideshare.net/jchoi7s/emotion-detection-on-tv-show-transcripts-with-sequencebased-convolutional-neural-networks},
	Year = {2018},
	Bdsk-Url-1 = {http://arxiv.org/pdf/1604.00938v1.pdf}}

@jurthesis{shaban:17b,
	Abstract = {This thesis introduces a subtask of entity linking, called character identification, that maps mentions in multiparty conversation to their referent characters. Transcripts of TV shows are collected as the sources of our corpus and automatically annotated with mentions by linguistically-motivated rules. These mentions are manually linked to their referents and disambiguate with abstract referent labels through crowdsourcing. Our annotated corpus comprises 448 scenes from 2 seasons and 46 episodes of the TV show Friends, and shows the inter-annotator agreement of kappa = 79.96. For statistical modeling, this task is reformulated as coreference resolution, and experimented with two state-of-the-art systems on our corpus. A novel mention-to-mention ranking model is proposed to provides better mention and mention-pair representations learned from feature groupings of dialogue-specific features After linking coreferent clusters to their referent entity with our proposed rule-based remap- ping algorithm, the best model gives a purity score of 57.27% on average, which is promising given the challenging nature of this task and our corpus.},
	Address = {Atlanta, GA},
	Author = {Shaban, Tarrek},
	Date-Added = {2017-07-13 02:56:51 +0000},
	Date-Modified = {2019-05-28 14:07:34 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2017.},
	School = {Emory University},
	Title = {{Trumping the Polls: Event Analysis During the 2016 Presidential Election}},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/jd472w54s},
	Url_Slides = {https://www.slideshare.net/jchoi7s/trumping-the-polls-event-analysis-during-the-2016-presidential-election},
	Year = {2017},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:pjw0g}}

@inproceedings{shin:17b,
	Abstract = {With the advent of word embeddings, lex- icons are no longer fully utilized for sentiment analysis although they still provide important features in the traditional setting. This paper introduces a novel approach to sentiment analysis that integrates lexicon embeddings and an attention mechanism into Convolutional Neural Networks. Our approach performs separate convolutions for word and lexicon embeddings and provides a global view of the document using attention. Our models are experimented on both the SemEval'16 Task 4 dataset and the Stanford Sentiment Treebank, and show comparative or better results against the existing state-of-the-art systems. Our analysis shows that lexicon embeddings allow to build high-performing models with much smaller word embeddings, and the attention mechanism effectively dims out noisy words for sentiment analysis.},
	Address = {Copenhagen, Denmark},
	Author = {Shin, Bonggun and Lee, Timothy and Choi, Jinho D.},
	Booktitle = {Proceedings of the EMNLP Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis},
	Date-Added = {2017-07-12 16:20:09 +0000},
	Date-Modified = {2017-11-30 18:51:25 +0000},
	Keywords = {emorynlp,selected},
	Pages = {149--158},
	Series = {WASSA'17},
	Title = {{Lexicon Integrated CNN Models with Attention for Sentiment Analysis}},
	Url = {http://optima.jrc.it/wassa2017/},
	Url_Paper = {http://www.aclweb.org/anthology/W/W17/W17-5220.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/lexicon-integrated-cnn-models-with-attention-for-sentiment-analysis},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1610.06272}}

@inproceedings{jurczyk:17a,
	Abstract = {This paper challenges a cross-genre document retrieval task, where the queries are in formal writing and the target documents are in conversational writing. In this task, a query, is a sentence extracted from either a summary or a plot of an episode in a TV show, and the target document consists of transcripts from the corresponding episode. To establish a strong baseline, we employ the current state-of-the-art search engine to perform document retrieval on the dataset collected for this work. We then introduce a structure reranking approach to improve the initial ranking by utilizing syntactic and semantic structures generated by NLP tools. Our evaluation shows an improvement of more than 4% when the structure reranking is applied, which is very promising.},
	Address = {Copenhagen, Denmark},
	Author = {Jurczyk, Tomasz and Choi, J. D.},
	Booktitle = {Proceedings of the EMNLP Workshop on Building Linguistically Generalizable NLP Systems},
	Date-Added = {2017-07-07 20:15:40 +0000},
	Date-Modified = {2017-11-30 18:46:27 +0000},
	Keywords = {emorynlp,selected,character-mining},
	Pages = {48--53},
	Series = {BLGNLP'17},
	Title = {{Cross-domain Document Retrieval: Matching between Conversational and Formal Writings}},
	Url = {http://generalizablenlp.weebly.com},
	Url_Paper = {http://www.aclweb.org/anthology/W17-5407.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/crossdomain-document-retrieval-matching-between-conversational-and-formal-writings},
	Year = {2017},
	Bdsk-Url-1 = {http://generalizablenlp.weebly.com}}

@inproceedings{shaban:17a,
	Abstract = {Since its introduction in 2006, Twitter has grown into an integral venue for political discourse. With this in mind, it is not surprising that Twitter and other social media services have played an important role in shaping the political debate during the 2016 presidential election. The dynamics of social media provide a unique opportunity to detect and interpret the pivotal events and scandals of the candidates quantitatively. This paper examines several text-based analysis to determine which topics have a lasting impact on the election for the two main candidates, Clinton and Trump. About 135.5 million tweets are collected over the six weeks prior to the election. From these tweets, topic clustering, keyword extraction, and tweeter analysis are performed to better understand the impact of the events occurred during this period. This analysis builds upon a social science foundation to provide another avenue for scholars to use in discerning how events detected from social media show the impacts of campaigns as well as campaign the election.},
	Address = {Oxford, UK},
	Author = {Shaban, Tarrek and Hexter, Lindsay and Choi, J. D.},
	Booktitle = {Proceedings of the 9th International Conference on Social Informatics},
	Date-Added = {2017-07-03 20:58:42 +0000},
	Date-Modified = {2017-11-30 18:44:17 +0000},
	Keywords = {emorynlp,selected},
	Pages = {184--200},
	Series = {SocInfo'17},
	Title = {{Event Analysis on the 2016 U.S. Presidential Election Using Social Media}},
	Url = {http://socinfo2017.oii.ox.ac.uk},
	Url_Paper = {https://link.springer.com/chapter/10.1007/978-3-319-67217-5_13},
	Url_Slides = {https://www.slideshare.net/jchoi7s/event-analysis-on-the-2016-us-presidential-election-using-social-media},
	Volume = {10539},
	Year = {2017},
	Bdsk-Url-1 = {http://socinfo2017.oii.ox.ac.uk}}

@inproceedings{jang:17a,
	Abstract = {Technical documents contain a fair amount of unnatural language, such as tables, formulas, and pseudo-code. Unnatural language can be an important factor of confusing existing NLP tools. This paper presents an effective method of distinguishing unnatural language from natural language, and evaluates the impact of unnatural language detection on NLP tasks such as document clustering. We view this problem as an information extraction task and build a multiclass classification model identifying unnatural language components into four categories. First, we create a new annotated corpus by collecting slides and papers in various formats, PPT, PDF, and HTML, where unnatural language components are annotated into four categories. We then explore features available from plain text to build a statistical model that can handle any format as long as it is converted into plain text. Our experiments show that removing unnatural language components gives an absolute improvement in document clustering by up to 15%. Our corpus and tool are publicly available.
},
	Address = {Copenhagen, Denmark},
	Author = {Jang, Myungha and Choi, Jinho D. and Allan, James},
	Booktitle = {Proceedings of the EMNLP Workshop on Noisy User-generated Text},
	Date-Added = {2017-06-08 14:28:11 +0000},
	Date-Modified = {2017-11-30 18:55:18 +0000},
	Keywords = {selected},
	Pages = {122--130},
	Series = {W-NUT'17},
	Title = {{Improving Document Clustering by Eliminating Unnatural Language}},
	Url = {http://noisy-text.github.io/2017/},
	Url_Paper = {http://www.aclweb.org/anthology/W/W17/W17-4416.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/improving-document-clustering-by-eliminating-unnatural-language},
	Year = {2017},
	Bdsk-Url-1 = {https://arxiv.org/abs/1703.05706}}

@inproceedings{chen:17b,
	Abstract = {This paper presents a novel approach to character identification, that is an entity linking task that maps mentions to characters in dialogues from TV show transcripts. We first augment and correct several cases of annotation errors in an existing corpus so the corpus is clearer and cleaner for statistical learning. We also introduce the agglomerative convolutional neural network that takes groups of features and learns mention and mention-pair embeddings for coreference resolution. We then propose another neural model that employs the embeddings learned and creates cluster embeddings for entity linking. Our coreference resolution model shows comparable results to other state-of-the-art systems. Our entity linking model significantly outperforms the previous work, showing the F1 score of 86.76% and the accuracy of 95.30% for character identification.},
	Address = {Vancouver, Canada},
	Author = {Chen, Henry Yu-Hsin and Zhou, Ethan and Choi, Jinho D.},
	Booktitle = {Proceedings of the 21st Conference on Computational Natural Language Learning},
	Date-Added = {2017-06-01 09:32:27 +0000},
	Date-Modified = {2017-08-31 13:34:00 +0000},
	Keywords = {emorynlp,selected,character-mining},
	Pages = {216--225},
	Series = {CoNLL'17},
	Title = {{Robust Coreference Resolution and Entity Linking on Dialogues: Character Identification on TV Show Transcripts}},
	Url = {http://www.conll.org/2017},
	Url_Paper = {http://www.aclweb.org/anthology/K/K17/K17-1023.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/robust-coreference-resolution-and-entity-linking-on-dialogues-character-identification-on-tv-show-transcripts},
	Year = {2017},
	Bdsk-Url-1 = {http://www.conll.org/2017}}

@inproceedings{ma:17a,
	Abstract = {We propose a convolutional neural network model for text-based speaker identification on multiparty dialogues extracted from the TV show, Friends. While most previous works on this task rely heavily on acoustic features, our approach attempts to identify speakers in dialogues using their speech patterns as captured by transcriptions to the TV show. It has been shown that different individual speakers exhibit distinct idiolec- tal styles. Several convolutional neural network models are developed to discriminate between differing speech patterns. Our results confirm the promise of text-based ap- proaches, with the best performing model showing an accuracy improvement of over 6% upon the baseline CNN model.},
	Address = {Vancouver, Canada},
	Author = {Ma, Kaixin and Xiao, Catherine and Choi, Jinho D.},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	Date-Added = {2017-06-01 09:25:16 +0000},
	Date-Modified = {2017-11-30 18:47:28 +0000},
	Keywords = {emorynlp,selected,character-mining},
	Pages = {49--55},
	Series = {ACL:SRW'17},
	Title = {{Text-based Speaker Identification on Multiparty Dialogues Using Multi-document Convolutional Neural Networks}},
	Url = {http://acl2017.org},
	Url_Paper = {http://aclweb.org/anthology/P17-3009.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/textbased-speaker-identification-on-multiparty-dialogues-using-multidocument-convolutional-neural-networks},
	Year = {2017},
	Bdsk-Url-1 = {http://acl2017.org}}

@jurthesis{chen:17a,
	Abstract = {This thesis introduces a subtask of entity linking, called character identification, that maps mentions in multiparty conversation to their referent characters. Transcripts of TV shows are collected as the sources of our corpus and automatically annotated with mentions by linguistically-motivated rules. These mentions are manually linked to their referents and disambiguate with abstract referent labels through crowdsourcing. Our annotated corpus comprises 448 scenes from 2 seasons and 46 episodes of the TV show Friends, and shows the inter-annotator agreement of kappa = 79.96. For statistical modeling, this task is reformulated as coreference resolution, and experimented with two state-of-the-art systems on our corpus. A novel mention-to-mention ranking model is proposed to provides better mention and mention-pair representations learned from feature groupings of dialogue-specific features After linking coreferent clusters to their referent entity with our proposed rule-based remap- ping algorithm, the best model gives a purity score of 57.27% on average, which is promising given the challenging nature of this task and our corpus.},
	Address = {Atlanta, GA},
	Author = {Chen, Henry Yu-Hsin},
	Date-Added = {2017-04-17 16:05:12 +0000},
	Date-Modified = {2019-05-28 14:07:54 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2017.},
	School = {Emory University},
	Title = {{Character Identification on Multi-party Dialogues}},
	Url = {http://pid.emory.edu/ark:/25593/rxf6s},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/qr46r1039},
	Url_Slides = {https://www.slideshare.net/jchoi7s/character-identification-on-multiparty-dialogues},
	Year = {2017},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:pjw0g}}

@inproceedings{shin:17a,
	Abstract = {The electronic health record (EHR) contains a large amount of multi-dimensional and unstructured clinical data of significant operational and research value. Distinguished from previous studies, our approach embraces a double-annotated dataset and strays away from obscure ``black-box'' models to comprehensive deep learning models. In this paper, we present a novel neural attention mechanism that not only classifies clinically important findings. Specifically, convolutional neural networks (CNN) with attention analysis are used to classify radiology head computed tomography reports based on five categories that radiologists would account for in assessing acute and communicable findings in daily practice. The experiments show that our CNN attention models outperform non-neural models, especially when trained on a larger dataset. Our attention analysis demonstrates the intuition behind the classifier's decision by generating a heatmap that highlights attended terms used by the CNN model; this is valuable when potential downstream medical decisions are to be performed by human experts or the classifier information is to be used in cohort construction such as for epidemiological studies.},
	Address = {Anchorage, AK},
	Author = {Shin, Bonggun and Chokshi, Falgun H. and Lee, Timothy and Choi, Jinho D.},
	Booktitle = {Proceedings of the International Joint Conference on Neural Networks},
	Date-Added = {2017-02-04 19:52:34 +0000},
	Date-Modified = {2018-02-27 15:56:25 +0000},
	Keywords = {emorynlp,selected,medical},
	Pages = {4363--4370},
	Series = {IJCNN'17},
	Title = {{Classification of Radiology Reports Using Neural Attention Models}},
	Url = {http://www.ijcnn.org},
	Url_Paper = {http://ieeexplore.ieee.org/document/7966408/},
	Url_Slides = {https://www.slideshare.net/jchoi7s/classification-of-radiology-reports-using-neural-attention-models},
	Year = {2017},
	Bdsk-Url-1 = {http://www.ijcnn.org}}

@inproceedings{choi:17a,
	Abstract = {This paper presents a method for the automatic conversion of constituency trees into deep dependency graphs consisting of primary, secondary, and semantic relations. Our work is distinguished from previous work concerning the generation of shallow dependency trees such that it generates dependency graphs incorporating deep structures in which relations stay consistent regardless of their surface positions, and derives relations between out-of-domain arguments, caused by syntactic variations such as open clause, relative clause, or coordination, and their predicates so the complete argument structures are represented for both verbal and non-verbal predicates. Our deep dependency graph conversion recovers important argument relations that would be missed by dependency tree conversion, and merges syntactic and semantic relations into one unified representation, which can reduce the bundle of developing another layer of annotation dedicated for predicate argument structures. Our graph conversion method is applied to six corpora in English and generated over 4.6M dependency graphs covering 20 different genres.},
	Address = {Bloomington, IN},
	Author = {Choi, Jinho D.},
	Booktitle = {Proceedings of the 15th International Workshop on Treebanks and Linguistic Theories},
	Date-Added = {2016-12-12 07:33:44 +0000},
	Date-Modified = {2018-08-21 18:08:59 +0000},
	Keywords = {emorynlp,selected},
	Pages = {35--62},
	Series = {TLT'17},
	Title = {{Deep Dependency Graph Conversion in English}},
	Url = {http://cl.indiana.edu/tlt15/},
	Url_Paper = {http://ceur-ws.org/Vol-1779/04choi.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/deep-dependency-graph-conversion-in-english?qid=51886170-00bf-43f3-8b9f-e2f3d0151922&v=&b=&from_search=1},
	Year = {2017},
	Bdsk-Url-1 = {http://cl.indiana.edu/tlt15/}}

@unpublished{xiao:16a,
	Author = {Xiao, Catherine and Jiang, Hang and Choi, Jinho D.},
	Date-Added = {2016-10-25 00:49:09 +0000},
	Date-Modified = {2018-02-27 18:21:44 +0000},
	Keywords = {emorynlp},
	Note = {Scholarly Inquiry and Research at Emory (SIRE).},
	Organization = {Emory University},
	Title = {{Seq2seq Model to Tokenize the Chinese Language}},
	Url = {http://college.emory.edu/undergraduate-research},
	Url_Slides = {https://www.slideshare.net/jchoi7s/seq2seq-model-to-tokenize-the-chinese-language-89086568},
	Year = {2016},
	Bdsk-Url-1 = {http://college.emory.edu/undergraduate-research}}

@inproceedings{jurczyk:16a,
	Abstract = {This paper presents a new dataset to benchmark selection-based question answering. Our dataset contains contexts drawn from the ten most prevalent topics in the English Wikipedia. For the generation of a large, diverse, and challenging dataset, a new annotation scheme is proposed. Our annotation scheme involves a series of crowdsourcing tasks that can be easily followed by any researcher. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon. We hope that providing a large corpus will enable researchers to work towards more effective open-domain question answering.},
	Address = {San Jose, CA},
	Author = {Jurczyk, Tomasz and Zhai, Michael and Choi, Jinho D.},
	Booktitle = {Proceedings of the 28th International Conference on Tools with Artificial Intelligence},
	Date-Added = {2016-08-24 01:57:03 +0000},
	Date-Modified = {2018-02-02 14:46:09 +0000},
	Keywords = {emorynlp,selected},
	Pages = {820--827},
	Series = {ICTAI'16},
	Title = {{SelQA: A New Benchmark for Selection-based Question Answering}},
	Url = {http://ieeexplore.ieee.org/document/7814688/},
	Url_Paper = {https://arxiv.org/pdf/1606.08513.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/selqa-a-new-benchmark-for-selectionbased-question-answering},
	Year = {2016},
	Bdsk-Url-1 = {https://arxiv.org/abs/1606.08513}}

@inproceedings{chokshi:16a,
	Author = {Chokshi, Falgun H. and Lemmon, Andrew B. and Choi, Jinho D.},
	Booktitle = {Proceedings of the 102nd Scientific Assembly and Annual Meeting of the Radiological Society of North America},
	Date-Added = {2016-07-27 20:44:30 +0000},
	Date-Modified = {2018-07-10 16:46:31 +0000},
	Keywords = {emorynlp,selected,medical},
	Series = {RSNA'16},
	Title = {{Extraction of Acute Communicable Findings from Head CT Reports Using Natural Language Processing and Machine Learning: InterReader Agreement and Accuracy of Three Methods}},
	Url = {http://www.rsna.org/Annual-Meeting/},
	Year = {2016},
	Bdsk-Url-1 = {http://www.rsna.org/Annual-Meeting/}}

@unpublished{chen:15a,
	Author = {Chen, Henry Yu-Hsin and Choi, Jinho D.},
	Date-Added = {2016-07-13 02:16:36 +0000},
	Date-Modified = {2018-08-21 18:41:14 +0000},
	Keywords = {emorynlp},
	Note = {Summer Undergraduate Research Program at Emory (SURE).},
	Organization = {Emory University},
	Title = {{Unsupervised Main Entity Extraction from News Articles using Latent Variables}},
	Url = {http://college.emory.edu/undergraduate-research/summer-programs},
	Url_Slides = {https://www.slideshare.net/jchoi7s/unsupervised-main-entity-extraction-from-news-articles-using-latent-variables},
	Year = {2015},
	Bdsk-Url-1 = {http://college.emory.edu/undergraduate-research/summer-programs}}

@unpublished{jiang:16a,
	Author = {Jiang, Hang and Choi, Jinho D.},
	Date-Added = {2016-07-13 02:14:17 +0000},
	Date-Modified = {2018-02-27 18:20:08 +0000},
	Keywords = {emorynlp},
	Note = {Scholarly Inquiry and Research at Emory (SIRE).},
	Organization = {Emory University},
	Title = {{Chinese Grammar vs English Grammar in Universal Dependency}},
	Url = {http://college.emory.edu/undergraduate-research},
	Url_Slides = {https://www.slideshare.net/jchoi7s/chinese-grammar-vs-english-grammar-in-universal-dependency},
	Year = {2016},
	Bdsk-Url-1 = {http://college.emory.edu/undergraduate-research}}

@inproceedings{liu:15a,
	Abstract = {Community-based Question Answering (CQA) services allow users to find and share information by interacting with others. A key to the success of CQA services is the quality and timeliness of the responses that users get. With the increasing use of mobile devices, searchers increasingly expect to find more local and time-sensitive information, such as the current special at a cafe around the corner. Yet, few services provide such hyper-local and time-aware question answering. This requires intelligent content recommendation and careful use of notifications (e.g., recommending questions to only selected users). To explore these issues, we developed RealQA, a real-time CQA system with a mobile interface, and performed two user studies: a formative pilot study with the initial system design, and a more extensive study with the revised UI and algorithms. The research design combined qualitative survey analysis and quantitative behavior analysis under different con- ditions. We report our findings of the prevalent information needs and types of responses users provided, and of the effectiveness of the recommendation and notification strategies on user experience and satisfaction. Our system and findings offer insights and implications for designing real-time CQA systems, and provide a valuable platform for future research.},
	Address = {Atlanta, GA},
	Author = {Liu, Qiaoling and Jurczyk, Tomasz and Choi, Jinho D. and Agichtein, Eugene},
	Booktitle = {Proceedings of the 20th Conference on Intelligent User Interfaces},
	Date-Added = {2016-07-02 02:16:03 +0000},
	Date-Modified = {2018-02-27 15:58:55 +0000},
	Keywords = {emorynlp,selected},
	Pages = {50--61},
	Series = {iUI'15},
	Title = {{Real-Time Community Question Answering: Exploring Content Recommendation and User Notification Strategies}},
	Url = {http://iui.acm.org/2015/},
	Url_Paper = {http://dl.acm.org/citation.cfm?id=2701392},
	Url_Slides = {https://www.slideshare.net/jchoi7s/realtime-community-question-answering-exploring-content-recommendation-and-user-notification-strategies},
	Year = {2015},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=2701392}}

@techreport{jurczyk:15b,
	Abstract = {This paper presents a precursory yet novel approach to the question answering task using structural decomposition. Our system first generates linguistic structures such as syntactic and semantic trees from text, decomposes them into multiple fields, then indexes the terms in each field. For each question, it decomposes the question into multiple fields, measures the relevance score of each field to the indexed ones, then ranks all documents by their relevance scores and weights associated with the fields, where the weights are learned through statistical modeling. Our final model gives an abso- lute improvement of over 40% to the baseline approach using simple search for detecting documents containing answers.},
	Author = {Jurczyk, Tomasz and Choi, Jinho D.},
	Date-Added = {2016-07-02 02:16:03 +0000},
	Date-Modified = {2017-08-31 03:01:06 +0000},
	Institution = {ArXiv},
	Keywords = {emorynlp,selected},
	Number = {1604.00938},
	Title = {{Multi-Field Structural Decomposition for Question Answering}},
	Url = {http://arxiv.org/abs/1604.00938},
	Year = {2015},
	Bdsk-Url-1 = {http://arxiv.org/pdf/1604.00938v1.pdf}}

@jurthesis{hahn:16a,
	Abstract = {Word embedding has drastically changed the field of natural language processing and has become the norm for distributional semantic models. Previous methods for generating word embeddings did not take advantage of the semantic information in sentence structures. In this work we create a new approach to word embedding that leverages structural data from sentences to produce higher quality word embeddings. We also introduce a framework to evaluate word embeddings from any part of speech. We use this framework to assess the quality of word embeddings produced with different semantic contexts and show that sentence structure is rich with semantic information. Our evaluations show that our new word embeddings far out preform the original word embeddings in all parts of speech. Furthermore we examine the task of sentiment analysis in order to demonstrate the superiority of our system's word embeddings.},
	Address = {Atlanta, GA},
	Author = {Hahn, Meera Satya},
	Date-Added = {2016-07-01 22:41:00 +0000},
	Date-Modified = {2019-05-28 14:08:04 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2016.},
	School = {Emory University},
	Title = {{Advances in Methods and Evaluations for Distributional Semantic Models using Computational Lexicons}},
	Url = {http://pid.emory.edu/ark:/25593/rj67f},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/6969z141z},
	Url_Slides = {https://www.slideshare.net/jchoi7s/advances-in-methods-and-evaluations-for-distributional-semantic-models-using-computational-lexicons},
	Year = {2016},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj67f}}

@jurthesis{kilgore:16a,
	Abstract = {Distributional Semantic word representation allows Natural Language Processing systems to extract and model an immense amount of information about a language. This technique maps words into a high dimensional continuous space through the use of a single-layer neural network. This process has allowed for advances in many Natural Language Processing research areas and tasks. These representation models are evaluated with the use of analogy tests, questions of the form ``If a is to a' then b is to what?'' are answered by composing multiple word vectors and searching the vector space.

During the neural network training process, each word is examined as a member of its context. Generally, a word's context is considered to be the elements adjacent to it within a sentence. While some work has been conducted examining the effect of expanding this definition, very little exploration has been done in this area. Further, no inquiry has been conducted as to the specific linguistic competencies of these models or whether modifying their contexts impacts the information they extract.

In this paper we propose a thorough analysis of the various lexical and grammatical competencies of distributional semantic models. We aim to leverage analogy tests to evaluate the most advanced distributional model across 14 different types of linguistic relationships. With this information we will then be able to investigate as to whether modifying the training context renders any differences in quality across any of these categories. Ideally we will be able to identify approaches to training that increase precision in some specific linguistic categories, which will allow us to investigate whether these improvements can be combined by joining the information used in different training approaches to build a single, improved, model.},
	Address = {Atlanta, GA},
	Author = {Kilgore, Andrew Reid},
	Date-Added = {2016-07-01 22:39:56 +0000},
	Date-Modified = {2019-05-28 14:08:13 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2016.},
	School = {Emory University},
	Title = {{Categorical Evaluation for Advanced Distributional Semantic Models}},
	Url = {http://pid.emory.edu/ark:/25593/rj97r},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/mk61rh195},
	Url_Slides = {https://www.slideshare.net/jchoi7s/categorical-evaluation-for-advanced-distributional-semantic-models},
	Year = {2016},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:rj97r}}

@jurthesis{blodgett:15a,
	Abstract = {Computational semantics as a field includes many of the unsolved problems of Natural Language Processing. The need for innovation in this field has motivated much research in developing word and language models that better represent meaning and various concepts within semantics. This thesis is concerned specifically with measuring verb similarity and verb clustering, a task within this field. The goal is to develop representations of verbs that can accurately and viably be used to judge semantic similarity between verbs and to group verbs into classes that reflect their relatedness in meaning. Verb clustering - a task of distributing verbs into semantically related classes - has in previous research been shown to have applications in multiple tasks in Natural Language Processing including word sense disambiguation. This thesis will present and compare several methods of automatic acquisition of verb similarity, with a goal of allowing future applications of these methods in NLP tasks and to promote discoveries in how the mechanisms modeled by these methods relate to linguistics.

This paper presents several methods from verb clustering based on Latent Dirichlet Allocation - a probabilistic graphical model commonly used for topic modelling. We model verbs as collections of contextual features derived from latent classes. LDA, which is designed as a model for Bayesian inference of latent thematic categories, fits well to model verb classes based on linguistic context. We demonstrate Recursive LDA, a procedure of executing LDA iteratively to produce a hierarchical structure of classes. We test several linguistic features from syntax and lexical arguments of verbs with interest in identifying how informative each feature is. We evaluate all of our experiments against human judgments of similarity providing a novel method for evaluating semantic similarity metrics of word models. We test all of our data on a list of 3,000 most common English verbs.

We test our method against Word2Vec, a popular and recently developed word model using skip-gram feature vectors refined by deep learning. The results in this thesis will show that given the right features, our method of using LDA with linguistic features outperforms Word2Vec's data-driven statistical approach when weighed against human judgements.},
	Address = {Atlanta, GA},
	Author = {Blodgett, Austin James},
	Date-Added = {2016-07-01 22:34:42 +0000},
	Date-Modified = {2019-05-28 14:08:26 -0400},
	Keywords = {emorynlp},
	Note = {Undergraduate Honors Thesis, Emory University, Atlanta, GA, 2015.},
	School = {Emory University},
	Title = {{The Verbiverse: Creating a Verb Space with Comparative Methods of Distributional Semantics}},
	Url = {http://pid.emory.edu/ark:/25593/pjw0g},
	Url_Paper = {https://etd.library.emory.edu/concern/etds/j38607456},
	Year = {2015},
	Bdsk-Url-1 = {https://etd.library.emory.edu/view/record/pid/emory:pjw0g}}

@inproceedings{chen:16a,
	Abstract = {This paper introduces a subtask of entity linking, called character identification, that maps mentions in multiparty conversation to their referent characters. Transcripts of TV shows are collected as the sources of our corpus and automatically annotated with mentions by linguistically-motivated rules. These mentions are manually linked to their referents through crowdsourcing. Our corpus comprises 543 scenes from two TV shows, and shows the inter-annotator agreement of $\kappa$ = 79.96. For statistical modeling, this task is reformulated as coreference resolution, and experimented with a state-of-the-art system on our corpus. Our best model gives a purity score of 69.21 on average, which is promising given the challenging nature of this task and our corpus.},
	Address = {Los Angeles, CA},
	Author = {Chen, Henry Yu-Hsin and Choi, Jinho D.},
	Booktitle = {Proceedings of the 17th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
	Date-Added = {2016-07-01 00:39:48 +0000},
	Date-Modified = {2018-02-09 17:34:23 +0000},
	Keywords = {emorynlp,selected,character-mining},
	Pages = {90--100},
	Series = {SIGDIAL'16},
	Title = {{Character Identification on Multiparty Conversation: Identifying Mentions of Characters in TV Shows}},
	Url = {http://www.sigdial.org/workshops/conference17/},
	Url_Paper = {http://www.aclweb.org/anthology/W16-3612.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/character-identification-on-multiparty-conversation-identifying-mentions-of-characters-in-tv-shows},
	Year = {2016},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/W14-4301}}

@inproceedings{lee:16a,
	Abstract = {This paper introduces a new corpus, QA-It, for the classification of non-referential it. Our dataset is unique in a sense that it is annotated on question answer pairs collected from multiple genres, useful for developing advanced QA systems. Our annotation scheme makes clear distinctions between 4 types of it, providing guidelines for many erroneous cases. Several statistical models are built for the classification of it, showing encouraging results. To the best of our knowledge, this is the first time that such a corpus is created for question answering.},
	Address = {Berlin, Germany},
	Author = {Lee, Timothy and Lutz Alex and Choi, Jinho D.},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
	Date-Added = {2016-05-27 01:41:01 +0000},
	Date-Modified = {2018-02-27 15:45:19 +0000},
	Keywords = {emorynlp,selected},
	Pages = {132--137},
	Series = {ACL:SRW'16},
	Title = {{QA-It: Classifying Non-Referential It for Question Answer Pairs}},
	Url = {http://acl2016.org},
	Url_Paper = {http://aclweb.org/anthology/P16-3020.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/classifying-nonreferential-it-for-question-answer-pairs},
	Year = {2016},
	Bdsk-Url-1 = {a},
	Bdsk-Url-2 = {b}}

@inproceedings{choi:16a,
	Abstract = {We introduce a novel technique called dynamic feature induction that keeps inducing high dimensional features automatically until the feature space becomes `more' linearly separable. Dynamic feature induction searches for the feature combinations that give strong clues for distinguishing certain label pairs, and generates joint features from these combinations. These induced features are trained along with the primitive low dimensional features. Our approach was evaluated on two core NLP tasks, part-of-speech tagging and named entity recognition, and showed the state-of-the-art results for both tasks, achieving the accuracy of 97.64 and the F1-score of 91.00 respectively, with about a 25% increase in the feature space.},
	Address = {San Diego, CA},
	Author = {Choi, Jinho D.},
	Booktitle = {Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics},
	Date-Added = {2016-03-03 18:54:23 +0000},
	Date-Modified = {2018-02-27 15:43:32 +0000},
	Keywords = {emorynlp,selected},
	Pages = {271--281},
	Series = {NAACL'16},
	Title = {{Dynamic Feature Induction: The Last Gist to the State-of-the-Art}},
	Url = {http://naacl.org/naacl-hlt-2016/},
	Url_Paper = {https://aclweb.org/anthology/N/N16/N16-1031.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/dynamic-feature-induction-the-last-gist-to-the-stateoftheart},
	Year = {2016},
	Bdsk-Url-1 = {http://naacl.org/naacl-hlt-2016/}}

@inproceedings{zhai:16a,
	Abstract = {In this paper, we first analyze the semantic composition of word embeddings by cross-referencing their clusters with the manual lexical database, WordNet. We then evaluate a variety of word embedding approaches by comparing their contributions to two NLP tasks. Our experiments show that the word embedding clusters give high correlations to the synonym and hyponym sets in WordNet, and give 0.88% and 0.17% absolute improvements in accuracy to named entity recognition and part-of-speech tagging, respectively.},
	Address = {Phoenix, AZ},
	Author = {Zhai, Michael and Tan, Johnny and Choi, Jinho D.},
	Booktitle = {Proceedings of the 30th AAAI Conference on Artificial Intelligence: Student Abstract and Poster Program},
	Date-Added = {2015-12-10 20:20:44 +0000},
	Date-Modified = {2018-02-27 15:45:08 +0000},
	Keywords = {emorynlp,selected},
	Pages = {4282--4283},
	Series = {AAAI:SAP'16},
	Title = {{Intrinsic and Extrinsic Evaluations of Word Embeddings}},
	Url = {https://www.aaai.org/Conferences/AAAI/aaai16.php},
	Url_Paper = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12454/12257.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/intrinsic-and-extrinsic-evaluations-of-word-embeddings},
	Year = {2016},
	Bdsk-Url-1 = {http://www.aaai.org/Conferences/AAAI/2016/aaai16accepted-papers.pdf}}

@inproceedings{choi:15b,
	Abstract = {The last few years have seen a surge in the number of accurate, fast, publicly available dependency parsers. At the same time, the use of dependency parsing in NLP applications has increased. It can be difficult for a non-expert to select a good ``off-the-shelf'' parser. We present a comparative analysis of ten leading statistical dependency parsers on a multi-genre corpus of English. For our analysis, we developed a new web-based tool that gives a convenient way of comparing dependency parser outputs. Our analysis will help practitioners choose a parser to optimize their desired speed/accuracy trade-off, and our tool will help practitioners examine and compare parser output.},
	Address = {Beijing, China},
	Author = {Choi, Jinho D. and Stent, Amanda and Tetreault, Joel},
	Booktitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics},
	Date-Added = {2015-04-24 07:54:51 +0000},
	Date-Modified = {2018-02-03 03:42:16 +0000},
	Keywords = {emorynlp,selected},
	Pages = {387--396},
	Series = {ACL'15},
	Title = {It Depends: Dependency Parser Comparison Using A Web-based Evaluation Tool},
	Url = {http://acl2015.org},
	Url_Paper = {http://www.aclweb.org/anthology/P15-1038.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/it-depends-dependency-parser-comparison-using-a-webbased-evaluation-tool},
	Year = {2015},
	Bdsk-Url-1 = {http://acl2015.org}}

@inproceedings{choi:15a,
	Abstract = {This paper describes the overall framework of our question-answering system designed to answer various types of complex questions.  Our framework makes heavy use of natural language processing techniques for the retrieval, ranking, and generation of correct answers.  Our approach has been tested on answering arithmetic questions requiring logical reasoning as well as higher-order factoid questions aggregating information across different documents.},
	Address = {Atlanta, GA},
	Author = {Choi, Jinho D.},
	Booktitle = {Proceedings of the US-Korea Conference on Science, Technology and Entrepreneurship},
	Date-Added = {2015-04-15 19:44:53 +0000},
	Date-Modified = {2018-02-27 16:18:02 +0000},
	Keywords = {emorynlp,selected},
	Pages = {150},
	Series = {UKC'15},
	Title = {{HyperQA: A Framework for Complex Question-Answering}},
	Url = {http://ukc.ksea.org/UKC2015/},
	Url_Paper = {https://www.slideshare.net/jchoi7s/hyperqa-a-framework-for-complex-questionanswering},
	Year = {2015},
	Bdsk-Url-1 = {http://ukc.ksea.org}}

@inproceedings{jurczyk:15a,
	Abstract = {This paper suggests an architectural approach of representing knowledge graph for complex question-answering. There are four kinds of entity relations added to our knowledge graph: syntactic dependencies, semantic role labels, named entities, and coreference links, which can be effectively applied to answer complex questions. As a proof of concept, we demon- strate how our knowledge graph can be used to solve complex questions such as arithmetics. Our experiment shows a promising result on solving arithmetic questions, achieving the 3-folds cross-validation score of 71.75%.},
	Address = {Denver, CO},
	Author = {Jurczyk, Tomasz and Choi, Jinho D.},
	Booktitle = {Proceedings of the 14th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop},
	Date-Added = {2015-04-07 21:26:35 +0000},
	Date-Modified = {2018-02-27 16:20:27 +0000},
	Keywords = {emorynlp,selected},
	Pages = {140--146},
	Series = {NAACL:SRW'15},
	Title = {{Semantics-based Graph Approach to Complex Question-Answering}},
	Url = {http://naacl.org/naacl-hlt-2015/},
	Url_Paper = {http://aclweb.org/anthology/N15-2019.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/semanticsbased-graph-approach-to-complex-questionanswering},
	Year = {2015},
	Bdsk-Url-1 = {a},
	Bdsk-Url-2 = {b}}

@inproceedings{nie:15a,
	Abstract = {English, like many languages, uses a wide variety of ways to talk about the future, which makes the automatic identification of future reference a challenge. In this research we ex- tend Latent Dirichlet allocation (LDA) for use in the identification of future-referring sentences. Building off a set of hand-designed rules, we trained a ADAGRAD classifier to be able to automatically detect sentences referring to the future. Uni-bi-trigram and syntactic rule mixed feature was found to provide the highest accuracy. Latent Dirichlet Allocation (LDA) indicated the existence of four major categories of future orientation. Lastly, the results of these analyses were found to correlate with a range of behavioral measures, offering evidence in support of the psychological reality of the categories.},
	Address = {Denver, CO},
	Author = {Nie, Aiming and Shepard, Jason and Choi, Jinho D. and Copley, Bridget and Wolff, Phillip},
	Booktitle = {Proceedings of the 14th Annual Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop},
	Date-Added = {2015-04-07 19:38:59 +0000},
	Date-Modified = {2018-02-27 16:28:19 +0000},
	Keywords = {emorynlp,selected},
	Pages = {168--173},
	Series = {NAACL:SRW'15},
	Title = {{Computational Exploration of the Linguistic Structures of Future-Oriented Expression: Classification and Categorization}},
	Url = {http://naacl.org/naacl-hlt-2015/},
	Url_Paper = {http://www.aclweb.org/anthology/N15-2023.pdf},
	Url_Slides = {https://www.slideshare.net/jchoi7s/computational-exploration-of-the-linguistic-structures-of-futureoriented-expression-classification-and-categorization},
	Year = {2015},
	Bdsk-Url-1 = {http://naacl.org/naacl-hlt-2015/}}
